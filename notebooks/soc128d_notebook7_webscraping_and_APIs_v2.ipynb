{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sociology 128D: Mining Culture Through Text Data: Introduction to Social Data Science – Summer '22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 7: Web Scraping and APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is a big topic. There are a lot of reasons someone might want to scrape web content, but the reason applicable to this class is to get data that may be useful for answering questions about some social phenomena.\n",
    "\n",
    "People who provide web content are typically savvy to the existence of tools for web scraping. You can often find references to automated web scraping in a site's Terms of Use (or equivalent), which often prohibit automated scraping.\n",
    "\n",
    "I'll just make two points here. First, the desirability of the data on a site is probably positively correlated with how prohibitive it is to scrape it, which can be unfortunate. Often, data that are interesting to us also have monetary value, and sites don't like to give it away for free.\n",
    "\n",
    "Second, we should try to be clear about what we mean by \"web scraping.\" Regarding the second point, we are typically referring to accessing a website's content in a way that is mediated by a tool or set of tools that makes it qualitatively different from browsing the web normally. As we'll see in our first example using the `requests` library, this can be as simple as using a line of Python code to store a web search in memory, rather than rendering it directly and immediately in a browser. We can then view what we've scraped (e.g., rendered HTML), which wouldn't be much different from normal browsing. We could also save it, or save some feature or set of features we've extracted from it. Doing this repeatedly is typically where things become problematic.\n",
    "\n",
    "At the most basic level, repeatedly scraping a site (or some part of it) means making repeated requests of the site's servers. That can be a problem in itself. The first point above just adds to this: sites may also want to protect their data, and may make it available subject to terms that prohibit automated scraping. Content is also served in different ways. Static websites are much easier to scrape than dynamic ones, which require a different approach.\n",
    "\n",
    "One compromise many sites make is to offer an application programming interface (API). In this notebook, we're going to keep our focus on getting data that may be useful for answering social research questions. Toward that end, we'll explore scraping static web content with an eye toward getting Twitter user handles for members of the US senate. Finally, we'll use an API to access data archived from Reddit.\n",
    "\n",
    "To clarify the purpose of this notebook, I want to draw attention to one of the early points made in the [Luscombe et al. (2022) reading](https://doi.org/10.1007/s11135-021-01164-0):\n",
    "\n",
    "> In practice, scraping is often closer to an art than a science, and can take years of practice to master (Possler\n",
    "et al. 2019). At the same time, it is a craft that requires continuous learning and problem\n",
    "solving, particularly as website development evolves and becomes ever more complex and\n",
    "thereby less accessible using existing tools. (p. 1024)\n",
    "\n",
    "Websites are structured very differently, so it is often the case that code for scraping must be tailored to a particular website. Additionally, websites change. Code that worked for a particular website at one point in time may stop working if the website changes. This notebook is meant to give students without any experience with web scraping a gentle introduction to scraping static content so that you can get a sense of whether it is worth the trouble. This notebook will *not* provide you with code (or permission!) to scrape any and all of the sites you might be interested in. We will also go through an example of using an API, which is more likely to be directly useful for the class.\n",
    "\n",
    "In the short term (e.g., for class projects), it will be easier to simply download a corpus that is readily available. APIs may offer a middle ground between using a ready-made corpus and building a scraper, but some APIs are tricky to access and use. There are tools available for working with Twitter's API, for example, [but you must request access to the API for your project](https://developer.twitter.com/en/docs/twitter-api) and wait for approval, which isn't guaranteed. Reddit's official API also [has limitations relevant to this class](https://www.reddit.com/wiki/api-terms/) (for example, you must be of legal age to sign a contract).\n",
    "\n",
    "If you are interested in Reddit data, one excellent resource is [pushshift.io](https://pushshift.io/). pushshift.io archives data from sites like Reddit in addition to providing an API for more specific searches. If you are interested in submissions to Reddit, for example, you can download them as bulk files for individual months throughout Reddit's history. However, the files from recent years have gotten to be quite large and may be difficult to work with if you don't have a large hard drive with a lot of available space. The pushshift API can help you get content that is more directly related to your research question–for example, submissions to specific subreddits from within a specific period of time–but there are some downsides to working with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For this notebook, you'll need to install `beautifulsoup4` and `psaw`.\n",
    "\n",
    "If you use Anaconda, you can install `beautifulsoup4` by running the following line in the Anaconda interpreter:\n",
    "\n",
    "```\n",
    "conda install -c anaconda beautifulsoup4 \n",
    "```\n",
    "\n",
    "Otherwise, you install it using `pip`. You will need to install `psaw` using pip regardless. (Depending on your setup, you may need to use `pip3` instead.)\n",
    "\n",
    "```\n",
    "pip3 install --user beautifulsoup4\n",
    "pip3 install --user psaw\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display, HTML\n",
    "from psaw import PushshiftAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping with Requests and BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1. Rendering Search Results inside Jupyter\n",
    "\n",
    "At its most basic level, \"scraping the web\" is just using a computer to access web content in a different way. The next two cells show how we can use the `requests` library to store the results of a web search in memory (in a variable we'll call <tt>results</tt>), which we can then render inside the notebook.\n",
    "\n",
    "We'll use `requests.get()` to get the web content we want to examine. The [`requests` library](https://docs.python-requests.org/en/master/) enables us to make HTTP requests, even with authentication.\n",
    "\n",
    "Running the second cell may change the way the notebook is displayed. You can comment it out and run the cell again if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.google.com/search?q=weather+stanford\"\n",
    "results = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(results.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2. Scraping Quotes from a Scraping Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of how scraping static content works, we'll start with a sandbox designed for this purpose. https://toscrape.com/ offers a couple of environments, including a [fictional bookstore](https://books.toscrape.com/). Since this is a class on text analysis, we're going to take a look at [another page](https://quotes.toscrape.com/), which displays quotes. When we make a request, we're hoping for a [response code of 200](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200). You can read more about other response codes [here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://quotes.toscrape.com/\"\n",
    "quotes_page = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_page.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that we can interact with the result like it's a string. If you type \"quotes_page.\" (ending with a period) and press the `tab` key, Jupyter will list several attributes you can explore, like the status code and headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quotes_page.text[:500]) # first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_page.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_page.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to parse the result and find the content we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(quotes_page.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now search the <tt>soup</tt> for all kinds of content. If you type \"soup.\" (ending with a period) in a Code cell and press the `tab` key, Jupyter will show different attributes or methods that are available.\n",
    "\n",
    "At this stage, scraping benefits from some knowledge of HTML. BeautifulSoup will allow us to access pieces of the page we scraped using various tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we print one `div` section (a chunk of the HTML) that shows a single quote and the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(soup.prettify()[600:1538])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.find_all()` method can be used for various types of content. Here we use it to get all of the `div` tags containing quotes. We then use `.find_all()` on each result to find the `span` tags nested inside. We use Python's `str.replace()` method to get rid of some unwanted text specific to this example and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first result for the \"quote\" class\n",
    "for div in soup.find_all(class_=\"quote\"):\n",
    "    print(div)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first span in the first div\n",
    "for div in soup.find_all(class_=\"quote\"):\n",
    "    for span in div.find_all(\"span\"):\n",
    "        print(span) #.text.replace(\"(about)\", \"\"))\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the text we want from inside each span in a quote-class div\n",
    "for div in soup.find_all(class_=\"quote\"):\n",
    "    for span in div.find_all(\"span\"):\n",
    "        print(span.text.replace(\"(about)\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3. Something Useful: Identifying Twitter Handles of Members of the Senate\n",
    "\n",
    "As we've noted, at its most basic level scraping is just accessing a site. Here we will scrape a \"real\" website–but we are only going to make *one* request. Specifically, we'll get the Twitter handles (along with state and party) of each current US senator from a site maintained by the UC San Diego Library. (Please be respectful of the site and don't spam them with requests!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ucsd.libguides.com/congress_twitter/senators\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senate_page = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(senate_page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(senate_page.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare the way the HTML is printed when using `.prettify()` on <tt>soup</tt> to printing the text from the original result from `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you explore the site in a browser or just scroll through the <tt>soup</tt>, you can see that the names, states, parties, and Twitter handles of the senators are arranged in a table, which is convenient for us. We'll use `.find_all()` to identify the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all(\"table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = soup.find_all(\"table\")\n",
    "for table in tables:\n",
    "    print(type(table), len(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that the info we want is inside `tr` tags, which are rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(tables[0])[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information we want for each senator (name, handle, state, and party) is contained in one row. The handle is in the URL of the `a` tag, while the senator's name is in the text of that tag. The state and party are in additional `td` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0].findAll(\"tr\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use [`enumerate()` (guide)](https://pythonbasics.org/enumerate/) with a for loop just to look at the first few results.\n",
    "\n",
    "This code finds all of the `tr` tags, ignores any without a link (e.g., to a Twitter account), finds all of the elements of the `ck_border` class, and prints the text. This prints the senator's name, state, and party. The `a` tag's attributes are like a dictionary, and the value for the key \"href\" is the URL to the senator's Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in enumerate(soup.find_all(\"tr\")):\n",
    "    if i < 4:\n",
    "        if result.a:\n",
    "            for element in result.find_all(class_=\"ck_border\"):\n",
    "                print(element.text)\n",
    "            print(result.a.attrs[\"href\"])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have figured out the way the information is structured, we will extract the name, state, party, and Twitter handle for each US senator. We'll create an empty list called <tt>senator_data</tt> to store the data initially. We'll use a nested for loop just like the one above, for we'll append each senator's name, state, party, and handle to a list called <tt>row</tt> before appending that row–one per senator–to <tt>senator_data</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senator_data = []\n",
    "\n",
    "for result in soup.find_all(\"tr\"):\n",
    "    if result.a:\n",
    "        row = []\n",
    "        for element in result.find_all(class_=\"ck_border\"):\n",
    "            row.append(element.text)\n",
    "        handle = result.a.attrs[\"href\"]\n",
    "        handle = handle.replace(\"https://twitter.com/\", \"\")\n",
    "        row.append(handle)\n",
    "        senator_data.append(row)\n",
    "    else:\n",
    "        print(result) # show the rows that aren't added to the dataset we're making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senator_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(senator_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a pandas dataframe from this list of lists. The `columns` argument lets us name the columns in the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(senator_data, columns=[\"senator\", \"state\", \"party\", \"twitter_handle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"senate_twitter_dataframe.csv\", index=None) # save the dataframe as a CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Reddit Content using `psaw`\n",
    "\n",
    "Another amazing resource for social media data is [pushshift.io](https://pushshift.io/), which archives vast amounts of data and makes it easily accessible. We'll use the [`psaw` library](https://github.com/dmarx/psaw) to access content from the pushshift.io Reddit API. Please be respectful of the service that pushshift.io offers. For example, if you want to use the API to get your own data, please request only a small amount of data first so that you can prototype everything, then request only as much data as you need and do so at a moderate pace. You may be temporarily blocked from using the API if you request too much too fast.\n",
    "\n",
    "For this example, we'll get posts to r/WallStreetBets from the last week of January, 2021. During this time, there was a lot of excitement about the rise of the GameStop stock–and then trading was halted on some platforms, [such as Robinhood](https://www.reuters.com/business/us-congress-hold-hearings-gamestop-trading-state-stock-markets-2021-01-28/).\n",
    "\n",
    "First, create an instance of the `PushShiftAPI()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the helper function <tt>get_results_from_pushshift()</tt> to turn the results we get into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_pushshift(subreddit: str, start_epoch, before_epoch, limit=10):\n",
    "    \"\"\"Fetches `limit` submissions to `subreddit` between `start_epoch` and `before_epoch`\"\"\"\n",
    "    results = list(api.search_submissions(after=start_epoch,\n",
    "                                      before=before_epoch,\n",
    "                                      subreddit=subreddit,\n",
    "                                      limit=limit))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "wsb = []\n",
    "\n",
    "year = 2020\n",
    "month = 1\n",
    "days = range(24,31)\n",
    "\n",
    "epochs = []\n",
    "\n",
    "for day in days:\n",
    "    start_epoch=int(dt.datetime(year, month, day).timestamp())\n",
    "    try:\n",
    "        before_epoch=int(dt.datetime(year, month, day+1).timestamp())\n",
    "    except:\n",
    "        before_epoch=int(dt.datetime(year, month+1, 1).timestamp()) # first day of next month\n",
    "        \n",
    "    epochs.append((start_epoch, before_epoch))\n",
    "    results = get_results_from_pushshift(\"WallStreetBets\", start_epoch, before_epoch)\n",
    "    wsb.append(results)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_flat = [post for sublist in wsb for post in sublist] # turn the list of lists into list of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wsb_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_df = pd.DataFrame([post.d_ for post in wsb_flat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_df[[\"author\", \"title\", \"selftext\", \"score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This event spawned a number of related subreddits. I have not personally followed these closely, but there is a lot there could be looked at sociologically, such as the dynamics of what could be considered a social movement or particular beliefs (and/or language) about certain stocks, companies, and regulatory agencies.\n",
    "\n",
    "You can modify the code above to look at different subreddits or periods of time. The code cell we use to collect the data is only looking at one month (January) in one year (2021), so there is a single loop that iterates through specific days. If you want to look at multiple months or years, you can nest for loops and iterate through those. Just bear in the mind the amount of data you are requesting!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
