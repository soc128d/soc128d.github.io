{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sociology 128D: Mining Culture Through Text Data: Introduction to Social Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2: Manipulating, Quantifying, and Visualizating Text Data\n",
    "\n",
    "#### A quick note about this notebook versus subsequent notebooks\n",
    "\n",
    "As a reminder, this class is meant to be a bit like a series of workshops, many of which could be self-contained. If you feel a bit lost amid all the code, that's okay--and if this all strikes you as incredibly basic, that's okay, too! Subsequent notebooks will use various methods to analyze a corpus in a way that can produce sociologically-interesting knowledge. These notebooks will be accessible even if you don't feel confident in your ability to follow the code because the focus won't be on the coding. The notebooks should also be interesting even if the code seems pretty basic to you because we'll be looking at applications to social research.\n",
    "\n",
    "#### A quick note about this notebook\n",
    "\n",
    "In this notebook, we'll begin to look at how to use libraries like `pandas` to manage and manipulate text. We'll also build toward different ways of quantifying information about documents and visualizing the results.\n",
    "\n",
    "This is only the second notebook, and this is all preliminary material. From here, we'll move on to measures like TF-IDF that can be used for information retrieval (e.g., search engines) and measuring document similarity, which has all sorts of applications in the social sciences. TF-IDF is also a jumping off point for other measures we'll consider, including dense vector representations.\n",
    "\n",
    "Let's have an initial go at using `pandas` with a dataset of tweets from January 6, 2021. You can download the dataset from Kaggle at [this link](https://www.kaggle.com/mrmorj/capitol-riot-tweets). Just unzip/extract the archive and place the CSV in the same directory as this notebook (or edit the `f = \"...\"` line in the following cell to include the file path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "f = \"tweets_2021-01-06.csv\"\n",
    "df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.head()` shows the first few rows. You can supply a number to see more or fewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.columns` lists the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.shape` shows the \"shape\" of the dataframe, meaning the number of rows followed by the number of columns. The following line of code shows there are 82,309 tweets (rows) and 14 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subset the dataset by column by supplying a list of the columns we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\"tweet_id\", \"text\", \"query\", \"follower_count\", \"likes\", \"retweets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also subset the **rows** using criteria like specific values for specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"tweet_id\"]==1346863072435179520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"likes\"] > 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"query\"]==\"mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also combine criteria to select rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"query\"]==\"mask\") & (df[\"retweets\"]==0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` has a lot of built-in functionality for manipulating, summarizing, and plotting data. Let's look at how we can calculate and plot the counts and means of different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"query\", \"tweet_id\"]].groupby(\"query\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"query\", \"tweet_id\"]].groupby(\"query\").count().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"query\", \"retweets\"]].groupby(\"query\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"query\", \"retweets\"]].groupby(\"query\").mean().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can use methods like `str.contains()` or even `if` statements like `if \"str\" in string: ...` when we're operating on strings, like below, sometimes we need to find other ways to interact with strings in pandas dataframes.\n",
    "\n",
    "In the following cell, there should be a `TypeError` when the we check whether the string \"capitol\" is in the \"text\" column of a row that apparently has a float instead. This is actually a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if \"capitol\" in row[\"text\"]:\n",
    "        COUNT += 1\n",
    "        \n",
    "print(COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        if \"capitol\" in row[\"text\"]:\n",
    "            COUNT += 1\n",
    "    except TypeError:\n",
    "        print(f\"Problematic type and value: {type(row['text'])}, {row['text']}\")\n",
    "        \n",
    "print(COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop the row with the missing value. The [`.dropna()` method](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html?highlight=dropna) can be applied to the entire row, but we may not care if data are missing for a variable that is not important to our analysis. We can use the `subset` argument to specify the columns we want `.dropna()` to look at. The following line means that we will drop any rows with missing data for the \"text\" column. We lose only one row this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset = [\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can count the number of tweets containing the string \"capitol\" at least once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNT = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if \"capitol\" in row[\"text\"]:\n",
    "        COUNT += 1\n",
    "        \n",
    "print(COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to take a quick detour to think about [lambda functions](https://www.geeksforgeeks.org/python-lambda-anonymous-functions-filter-map-reduce/), which are functions that we define in just one line of code. As we'll see in a moment, these are useful for operating on columns in pandas dataframes. Let's also look at [list comprehensions](https://www.geeksforgeeks.org/python-list-comprehension/), which allow us to execute a loop in just one line of code.\n",
    "\n",
    "The for loop below appends the values 0-9 to a list called `integers`. In the next cell, we use a list comprehension to do the same thing, and both print statements match. List comprehensions can be faster and actually quite flexible, but they can also end up being difficult to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integers = []\n",
    "\n",
    "for i in range(10):\n",
    "    integers.append(i)\n",
    "    \n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integers = [i for i in range(10)]\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two cells, we see a function defined in a relatively conventional way. This function takes in the variable `x`, which should be an integer, and multiplies it by 10, returning another integer.\n",
    "\n",
    "The second cell defines an equivalent function in only one line. `lambda x:` means we're about to do something to `x`, for any value of `x` that is supplied. The part after the colon is what we do to `x`, which in this case is multiplying it by 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_by_ten(x: int) -> int:\n",
    "    return x * 10\n",
    "\n",
    "integers = [i for i in range(10)]\n",
    "integers = [multiply_by_ten(x) for x in integers]\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integers = [i for i in range(10)]\n",
    "\n",
    "print(f\"Original list: {integers}\")\n",
    "\n",
    "multiply_by_ten = lambda x: x*10\n",
    "\n",
    "integers = [multiply_by_ten(x) for x in integers]\n",
    "\n",
    "print(f\"After applying lambda function: {integers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use those in a moment. Next, let's look at how we can iterate through a pandas column. We'll look at the \"text\" column as a list, then use a for loop to calculate the wordcounts. Then we'll define a function in the traditional way and apply it to the \"text\" column in the pandas dataframe directly using the `.apply()` method. Then we'll do the same thing using a lambda function instead of a function we've defined the conventional way. Finally, we'll show that these provide the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "for tweet in df[\"text\"].tolist():\n",
    "    tweets.append(tweet)\n",
    "    \n",
    "for i in range(5):\n",
    "    print(tweets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(tweets[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts = []\n",
    "\n",
    "for tweet in df[\"text\"].tolist():\n",
    "    wordcount = len(tweet.split())\n",
    "    wordcounts.append(wordcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pyplot](https://matplotlib.org/stable/tutorials/introductory/pyplot.html) and [seaborn](https://seaborn.pydata.org/) will be staples of data visualization in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_palette(\"flare\")\n",
    "\n",
    "sns.kdeplot(wordcounts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcount(word: str) -> int:\n",
    "    return len(word.split())\n",
    "\n",
    "\n",
    "df[\"wordcount\"] = df[\"text\"].apply(wordcount)\n",
    "\n",
    "sns.kdeplot(\"wordcount\", data = df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"wordcount2\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "sns.kdeplot(\"wordcount2\", data = df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can superimpose these distributions to show that they are the same (though we can do this other ways). We should just see one distribution in this case because they are being plotted over one another. If these were different variables, we would see multiple distributions in this plot, as we'll see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(\"wordcount\", data = df)\n",
    "sns.kdeplot(\"wordcount2\", data = df)\n",
    "sns.kdeplot(wordcounts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`collections` module](https://docs.python.org/3/library/collections.html) provides a lot useful methods for manipulating data. We will use `defaultdict()` a lot later on, for example. Right now, let's take a look at `Counter()` . Counter provides an object like a dictionary with the elements of an iterable (like a list) as keys and the frequencies as values. In the example below, we get a mapping of values from the \"query\" column (keys) to their frequencies in the dataframe (values), or the number of rows with those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "c = Counter(df[\"query\"])\n",
    "print(type(c))\n",
    "print(c.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at tweets from a subset of these queries. We'll subset the dataframe a bit like we did above, this time using the `.isin()` method.\n",
    "\n",
    "The code in the second cell, `df[df[\"query\"].isin(query_subset)]`, will return a dataframe matching that criterion. What's going on with this way of subsetting the data, though?\n",
    "\n",
    "As we can see in the third cell, if we execute just the code used to subset the data, `df[\"query\"].isin(query_subset)`, we get a pandas Series (like a list) of `True` and `False` of the same length as the number of rows in the dataframe. When we apply that condition to the dataframe to subset it in the second cell (`df[df[\"query\"].isin(query_subset)]`), we are return the rows where that condition is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_subset = [\"lockdown\", \"mask\", \"quarantine\", \"travel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"query\"].isin(query_subset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"query\"].isin(query_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df[\"query\"].isin(query_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(\"wordcount\", data = df[df[\"query\"].isin(query_subset)], hue = \"query\")\n",
    "plt.xlabel(\"Wordcount\")\n",
    "plt.title(\"Distribution of Wordcounts in Tweets by Query\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "sns.kdeplot(\"wordcount\", data = df[df[\"query\"].isin(query_subset)], hue = \"query\")\n",
    "plt.xlabel(\"Wordcount\")\n",
    "plt.title(\"Distribution of Wordcounts in Tweets by Query\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = \"wordcount\", y = \"retweets\", data = df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's subset the data so we can look at tweets containing the string \"capitol\" at least once. We'll define a function, `contains_capitol`, that takes in a string and returns `True` if the string contains \"capitol\" and `False` if it doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_capitol(text: str) -> bool:\n",
    "    if \"capitol\" in text:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "test_string1 = \"this string contains capitol\"\n",
    "test_string2 = \"this string does not\"\n",
    "\n",
    "print(contains_capitol(test_string1))\n",
    "print(contains_capitol(test_string2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a variable using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"contains_capitol\"] = df[\"text\"].apply(contains_capitol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(df[\"contains_capitol\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's subset the rows to keep only the tweets containing \"capitol\" (in the next cell) and then drop our new column and the duplicate wordcount column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"contains_capitol\"]] # we don't need to specify \"== True\" because Python assumes this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"wordcount2\", \"contains_capitol\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a quick look at some properties of the wordcounts of this subset of tweets. We can control the precision when we print floats, including when we use [f-strings](https://www.geeksforgeeks.org/formatted-string-literals-f-strings-python/), like below. `.2f` restricts to the output to two digits after the decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average wordcount: {df['wordcount'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average wordcount: {df['wordcount'].mean():.2f}\")\n",
    "print(f\"Minimum wordcount: {df['wordcount'].min()}\")\n",
    "print(f\"Maximum wordcount: {df['wordcount'].max()}\")\n",
    "print(f\"Standard deviation: {df['wordcount'].std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note on calculating variances and standard deviations in Python and other languages:\n",
    "\n",
    "[The `std()` method from pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.std.html) divides by N - 1. This is similar to the default for R and Stata.\n",
    "\n",
    "[The `std()` method from numpy](https://numpy.org/doc/stable/reference/generated/numpy.std.html) divides by N. You can change this by using the `ddof` argument in either method (setting it to 0 or 1).\n",
    "\n",
    "If you're curious, see [here](https://en.wikipedia.org/wiki/Bessel%27s_correction) and [here](https://math.stackexchange.com/questions/2060400/why-do-you-use-n-1-in-standard-error-of-the-mean-but-n-in-hypothesis-testing) for more on why we often divide by N - 1 for the sample variance (and thus standard deviation), rather than by N (as we do for the population)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integers_as_strings = [str(i) for i in range(10)]\n",
    "print(f\"As a list: {integers_as_strings}\")\n",
    "\n",
    "integers_as_strings = \" \".join(integers_as_strings)\n",
    "print(f\"As a single string: {integers_as_strings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at term frequency and document frequency. We'll use these to build up to powerful ways to compare documents. One application is finding similar documents (e.g., in search engines). We'll see later in the quarter how these can also be used to compare things like online communities like subreddits.\n",
    "\n",
    "To get the frequencies of all the words in the corpus, we can quickly combine all of the tweets (the \"text\" column) using the `.join()` method, implicitly treating the column as a list of tweets. If we look at a slice of the document (by characters), we can see that we've just merged all of the tweets together, joining them with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \" \".join(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we `split` all of this on whitespace, we get the individual *tokens*, or instances of words. If we use `Counter`, this returns the number of times each `type` (*unique* token) occurs. See [here](https://en.wikipedia.org/wiki/Type%E2%80%93token_distinction) for the type-token distinction. Each word is a token, but it is also an instance of a unique type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = dict(Counter(all_text.split()))\n",
    "\n",
    "types_and_counts = sorted(list(word_frequencies.items()), reverse = True, key = lambda x: x[1])\n",
    "print(types_and_counts[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine the distribution of word frequencies. The code below \"unzips\" the types and frequencies into lists that stay in the same order. `types_` will be a list of the types in the original order, and `token_counts` will be a list of the frequencies in the same order. The first element in `types_` will be the most frequent, while the last will be the least frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_, token_counts = zip(*types_and_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x = range(100), height = token_counts[:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x = types_[:20], height = token_counts[:20])\n",
    "plt.xticks(rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linguists will sometimes refer to [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) when discussing the distribution of words. You'll typically see that a few words are extremely common and most are extremely rare. Put one way, there is a negative correlation between the rank of a word (where 1 = most frequent) and its frequency.\n",
    "\n",
    "We'll talk about \"stop words\" more soon; these are extremely frequent words which are generally seen to have little semantic information (e.g., \"the\" or \"to\"). Looking at the distribution of words will also be relevant later when we start to look at tools like topic modeling, where some algorithms make assumptions about how words are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "log_rank = np.log(range(1, len(token_counts)+1))\n",
    "log_frequencies = np.log(token_counts)\n",
    "\n",
    "plt.plot(log_rank, log_frequencies)\n",
    "plt.ylabel(\"ln(word frequency)\")\n",
    "plt.xlabel(\"ln(word rank)\")\n",
    "plt.title(\"Word Rank versus Frequency (log-log)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(types_[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(types_[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(types_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(token_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dictionary, `word_frequencies`, that maps each type (unique word) to its frequency in the corpus. Let's make a dictionary mapping types to their *document frequency*, or the number of documents in which they occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_of_types(document: str) -> str:\n",
    "    return \" \".join(list(set(document.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"this is a string that repeats some words, like string and words and some\"\n",
    "\n",
    "Counter(s.split()) # three types occur twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = set_of_types(s)\n",
    "\n",
    "Counter(s2.split()) # each type occurs only once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we applly this function to each document, each row will have a string containing the types in the original tweet, but each will appear only once. If we count up the occurrences of each word in this column, it's the same as going through each row and checking whether the type occurs at least once. In other words, this helps us count the number of documents in which each word occurs. There are other ways to do this, of course, and later we'll see that we can use libraries like `scikit-learn` to do this more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"types\"] = df[\"text\"].apply(set_of_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_frequencies = dict(Counter(\" \".join(df[\"types\"]).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_frequencies[\"capitol\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does the type \"capitol\" occur in only 1,736 documents, even though there are 1,750 documents we subset based on whether they contained \"capitol\" as part of the overall string? The answer is that we didn't look for the *type* \"capitol\" but instead for whether the string \"capitol\" occurred in the overall tweet. If we iterate through the words in each tweet, we can find out which strings *contain* \"capitol\" but aren't themselves instances of the *type* \"capitol.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set()\n",
    "\n",
    "for tweet in df[\"types\"].tolist():\n",
    "    for word in tweet.split():\n",
    "        if \"capitol\" in word:\n",
    "            s.add(word)\n",
    "            \n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_frequencies_list = sorted(list(document_frequencies.items()), reverse = True, key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document_frequencies_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document_frequencies_list[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the corpus level, word frequency and document frequency are highly correlated. However, we will also look at the frequency of words *within* a document, which can help us identify similar documents. Documents that use the same words at approximately the same rates are similar in that important way. However, not all words are equally informative. As we saw above, some words are extremely common.\n",
    "\n",
    "If two sentence uses the word \"the\" many times, that doesn't tell us a lot. If the two documents use the word \"insurgents\" many times, relative to the typical document, then they likely share information we are interested in. This is a step toward saying the documents are \"about\" the same thing--if not quite saying that they \"mean\" something similar. In practice, if we are comparing documents based on the frequencies of the words they use, we will normalize the word counts in some way by the document frequency (i.e., how many documents a type appears in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "vocabulary = sorted(list(word_frequencies.keys()))\n",
    "\n",
    "x = [word_frequencies[word] for word in vocabulary]\n",
    "y = [document_frequencies[word] for word in vocabulary]\n",
    "\n",
    "print(\"Correlation between each word's frequency in the overall corpus and its document frequency:\")\n",
    "print(f\"Pearson's correlation coefficient: {pearsonr(x, y)[0]:.2f}\")\n",
    "print(f\"Spearman's rank-order correlation: {spearmanr(x, y)[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested in analyzing meaning from a corpus, in practice we will often remove words that appear only once or in only one document (which aren't the same thing!). We sometimes call these [hapaxes](https://en.wikipedia.org/wiki/Hapax_legomenon). We can't say that two documents have a word in common if only one document in the entire corpus has the word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_keep = [word for word in vocabulary if document_frequencies[word] > 1]\n",
    "print(len(words_to_keep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may often exclude words that appear in *every* document for similar reasons.\n",
    "\n",
    "Let's remove hapaxes from our dictionaries of word and document frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = {key:value for key, value in word_frequencies.items() if key in words_to_keep}\n",
    "document_frequencies = {key:value for key, value in document_frequencies.items() if key in words_to_keep}\n",
    "\n",
    "vocabulary = words_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_frequencies.keys()) == len(document_frequencies.keys()) == len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for Notebook 2! Now we're ready to talk about TF-IDF and other ways of comparing documents, which will set us up for a shift to tasks like comparing (or unmasking!) authors, comparing the meaning of words, identifying latent themes in documents, and using these kinds of features--information we have mined by quantifying properites of the text--to answer all manners of social research questions. These tools and skills will also transfer to other areas, so don't worry if studying culture isn't 100% your bag!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
