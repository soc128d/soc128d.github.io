{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sociology 128D: Mining Culture Through Text Data: Introduction to Social Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 8: Bias and Stereotyping in Pretrained Word Embeddings\n",
    "\n",
    "In this notebook, we will use pretrained word embeddings to develop intuitions about how word embedding models can be used to answer social research questions. We start with a brief introduction to word embeddings in general and a demonstration of solving analogies using word vectors.\n",
    "\n",
    "Next, we draw inspiration from [\"The Geometry of Culture\" (Kozlowski, Taddy, & Evans, 2019)](https://journals.sagepub.com/doi/full/10.1177/0003122419877135) and explore how word embedding models can be used to study cultural associations, for example classed, racialized, and gendered perceptions of music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Word Embeddings?\n",
    "\n",
    "Word embeddings are vectors (lists of numbers) that represent the positions of distinct words in a shared embedding space. They are similar to the tf-idf-weighted document vectors we used in [Notebook 5](https://soc128d.github.io/notebooks/) in that the unit of analysis is represented by a vector of numbers and these numbers reflect information about the co-occurrence of words in a corpus. In this case, however, the unit of analysis is the individual word, not a document.\n",
    "\n",
    "In general, word embeddings capture patterns of similar usage of words. Words used in similar contexts will be closer together in the embedding space. Synonyms are the extreme case: if you replace one synonym with another in a sentence (e.g., replace \"car\" with \"automobile\"), the meaning of the sentence may not change a great deal. Both synonyms could reasonably be used in that sentence, and both versions of that sentence (or sentences like it) could occur frequently in a corpus. The other words in the sentence are <tt>context words</tt> (and, in fact, every word is a context word for every other word in a given sentence). Words used frequently in the presence of the same context words appear closer together in the embedding space.\n",
    "\n",
    "In Notebook 5, we started with a document X term (word) matrix. For word embeddings, the inuitive starting point is a word X _context word_ matrix representing the number of times words co-occur (or are \"context words\" for one another). The issue is that such a matrix is often very sparse: it might be very large, but most words do not co-occur. Other approaches lead to dense vectors in which columns do not correspond to context words.\n",
    "\n",
    "We can use measures like cosine similarity to quantify the distance in the embedding space, and this reflects similarity in how words are used. We often make the further leap of assuming that this measures what words _mean_. [Sahlgren (2008, p. 1)](https://www.diva-portal.org/smash/get/diva2:1041938/FULLTEXT01.pdf) points to numerous formulations of this claim:\n",
    "\n",
    "> Distributional approaches to meaning acquisition utilize distributional properties of linguistic entities as the building blocks of semantics. In doing so, they rely fundamentally on a set of assumptions about the nature of language and meaning referred to as the distributional hypothesis. This hypothesis is often stated in terms like “words which are similar in meaning occur in similar contexts” (Rubenstein & Goodenough, 1965); “words with similar meanings will occur with similar neighbors if enough text material is available” (Sch ̈utze & Pedersen, 1995); “a representation that captures much of how words are used in natural context will capture much of what we mean by meaning” (Landauer & Dumais, 1997); and “words that occur in the same contexts tend to have similar meanings” (Pantel, 2005), just to quote a few representative examples. The general idea behind the distributional hypothesis seems clear enough: there is a correlation between distributional similarity and meaning similarity, which allows us to utilize the former in order to estimate the latter.\n",
    "\n",
    "There are a number of approaches to training word embeddings, but one of the most common--as we've seen in various readings--is word2vec. You can find an excellent illustrated guide to word2vec [here](https://jalammar.github.io/illustrated-word2vec/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You'll need a version of the `gensim` library. You can install `gensim` from the Anaconda interpreter using the following command:\n",
    "\n",
    "```\n",
    "conda install -c anaconda gensim\n",
    "```\n",
    "\n",
    "If you are not using Anaconda, you can use `pip` to install `gensim` from the command line:\n",
    "\n",
    "```\n",
    "pip3 install gensim\n",
    "```\n",
    "\n",
    "This notebook has been tested with versions 3.8.3 and 4.0.1. In the shift to version 4, [a few notable changes were made](https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4). For this notebook, the only difference that should matter is that word embedding models had a <tt>.vocab</tt> attribute in version 3, and this has been replaced with <tt>.key_to_index</tt> in version 4. There are a couple of lines of code in this notebook that use one of these attributes; they are flagged for you, and you can simply make sure to comment out the version you don't need and uncomment the version you do.\n",
    "\n",
    "Later in the notebook, you'll explore latent cultural dimensions measured as in [\"The Geometry of Culture\"](https://journals.sagepub.com/doi/full/10.1177/0003122419877135). You will create these using pairs of antonyms provided by the authors. The notebook includes links to these lists of word pairs and a function to load them as well as a function that will get them directly from the web. If you plan to use the word pairs on your own (e.g., for your project) or plan to run the notebook multiple times, it might be best to download the lists and use the function provided to load them. Otherwise, getting them directly from the web should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn`'s `cosine_similarity()` method expects vectors to be in a certain shape. The <tt>cosine_reshape()</tt> function below is a helper function to handle that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_reshape(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Reshape word vectors and return cosine\n",
    "    similarity\n",
    "    \"\"\"\n",
    "    if vec1.shape[0] > 1:\n",
    "        vec1 = vec1.reshape(1, -1)\n",
    "    if vec2.shape[0] > 1:\n",
    "        vec2 = vec2.reshape(1, -1)\n",
    "    assert vec1.shape == vec2.shape\n",
    "    return cosine_similarity(vec1, vec2)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load our word embedding model. The notebook defaults to [<tt>word2vec-google-news-300</tt>](https://github.com/RaRe-Technologies/gensim-data), which has a vocabulary of three million words, has three hundred dimensions, and was trained on a Google New corpus. You can try other pretrained embeddings if you prefer. `gensim` also vectors trained using algorithms like [GloVe](https://nlp.stanford.edu/projects/glove/), with versions trained on different types of data and with different numbers of dimensions. Typically, smaller models (i.e., with smaller vocabularies or fewer dimensions) are faster to work with, but may not represent the meaning of the words as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors = gensim.downloader.load(\"glove-wiki-gigaword-300\") # if you'd prefer to try GloVe vectors instead!\n",
    "word_vectors = gensim.downloader.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you have `gensim` 4.0, you'll need to comment out the lines with references to the <tt>.vocab</tt> attribute and uncomment the lines that reference to the <tt>.key_to_index</tt> attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(word_vectors.key_to_index)\n",
    "# type(word_vectors.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(word_vectors.key_to_index):,} words in the vocabulary.\")\n",
    "# print(f\"There are {len(word_vectors.vocab):,} words in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at an example of a word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors[\"sociology\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding for sociology indicates its position in a 300-dimensional space, but there is a vocabulary of three million words. The dimensions do not correspond to context words, and do not necessarily correspond to anything interpretable on their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_vectors[\"sociology\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity and Analogies\n",
    "\n",
    "`gensim` makes it easy to compare word embeddings. the `.similarity()` method takes two words as arguments and provides the cosine similarity of the word embeddings. This is the same measure of similarity we used for comparing document vectors.\n",
    "\n",
    "`gensim`'s `.most_similar()` method takes one word as an argument and provides the <tt>topn</tt> most similar words in the model. However, the `.most_similar()` method also allows you to add and subtract word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.similarity(\"sociology\", \"anthropology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(\"sociology\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical example of analogies with word vectors is finding a feminine counterpart of \"king\" by addition and subtraction of other vectors. Starting with the vector for king, we can subtract the vector for \"man\" and add the vector for \"woman.\"\n",
    "\n",
    "The analogy takes the following form:\n",
    "```\n",
    " man:king::woman:___\n",
    "```\n",
    "Using the `.most_similar()` method, we provide a `list` called <tt>positive</tt> that includes \"king\" and \"woman\" and a `list` called <tt>negative</tt> including only \"man.\" You can think of this as adding the vector for \"woman\" to the vector for \"king\" while subtracting the vector for \"man.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vectors can also seemingly pick up aspects of grammar:\n",
    "\n",
    "```\n",
    "wide:wider::light:___\n",
    "read:reading::listen:___\n",
    "write:wrote::listen:___\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=[\"wider\", \"light\"], negative=[\"wide\"], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=[\"reading\", \"listen\"], negative=[\"read\"], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=[\"wrote\", \"listen\"], negative=[\"write\"], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about when we subtract the vector for \"society\" from the vector for \"sociology\" and add the vector for \"cell\"?\n",
    "\n",
    "```\n",
    "society:sociology::cell:___\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=[\"sociology\", \"cell\"], negative=[\"society\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring \"Cultural Dimensions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you would prefer to download and save the lists of word pairs used by Kozlowski et al. (2019):\n",
    "\n",
    "Affluence pairs: https://raw.githubusercontent.com/KnowledgeLab/GeometryofCulture/master/code/word_pairs/affluence_pairs.csv\n",
    "\n",
    "Gender pairs: https://raw.githubusercontent.com/KnowledgeLab/GeometryofCulture/master/code/word_pairs/gender_pairs.csv\n",
    "\n",
    "Race pairs: https://raw.githubusercontent.com/KnowledgeLab/GeometryofCulture/master/code/word_pairs/race_pairs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pairs_from_file(f: str) -> list:\n",
    "    \"\"\"\n",
    "    Read in word pairs from `f` as list\n",
    "    \"\"\"\n",
    "    \n",
    "    pairs = open(f, \"r\").read().strip().split(\"\\n\")\n",
    "    pairs = [pair.split(\",\") for pair in pairs]\n",
    "    return [(pair[0], pair[1]) for pair in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_f = \"affluence_pairs.csv\"\n",
    "gender_f = \"gender_pairs.csv\"\n",
    "race_f = \"race_pairs.csv\"\n",
    "\n",
    "aff_pairs = read_pairs_from_file(aff_f)\n",
    "gender_pairs = read_pairs_from_file(gender_f)\n",
    "race_pairs = read_pairs_from_file(race_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you would prefer to get the lists of word pairs used by Kozlowski et al. (2019) using the `requests` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pairs_from_url(url: str) -> list:\n",
    "    \"\"\"\n",
    "    Get word pairs using requests library\n",
    "    \"\"\"\n",
    "    \n",
    "    pairs = requests.get(url)\n",
    "    pairs = pairs.text.strip().split(\"\\n\")\n",
    "    pairs = [pair.replace(\"\\r\", \"\").split(\",\") for pair in pairs]\n",
    "    return [(pair[0], pair[1]) for pair in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_url = \"https://raw.githubusercontent.com/KnowledgeLab/GeometryofCulture/master/code/word_pairs/affluence_pairs.csv\"\n",
    "gender_url = \"https://raw.githubusercontent.com/KnowledgeLab/GeometryofCulture/master/code/word_pairs/gender_pairs.csv\"\n",
    "race_url = \"https://raw.githubusercontent.com/KnowledgeLab/GeometryofCulture/master/code/word_pairs/race_pairs.csv\"\n",
    "\n",
    "aff_pairs = read_pairs_from_url(aff_url)\n",
    "gender_pairs = read_pairs_from_url(gender_url)\n",
    "race_pairs = read_pairs_from_url(race_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is a recreation of the <tt>make_dim()</tt> function in the original `R` code provided by Kozlowski et al. [here](https://github.com/KnowledgeLab/GeometryofCulture/blob/master/code/build_cultural_dimensions.R). Like other research on bias in word embeddings, this work presupposes cultural associations that can be represented as binaries or continua.\n",
    "\n",
    "To identify an \"affluence\" dimension, this approach uses the differences of vectors for antonyms like \"rich\" and \"poor\" (e.g., \"rich\" minus \"poor\").\n",
    "\n",
    "To identify \"gender\" and \"race\" dimensions, this approach makes the assumption that _culturally_ there are axes where the idea of gender or ethnic \"antonyms\" makes sense. If there is gender bias such that some things are stereotyped as being more associated with men than women (or vice versa), this approach may be able to detect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dim(word_pairs: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate a \"cultural dimension\" as in\n",
    "    https://github.com/KnowledgeLab/GeometryofCulture/blob/master/code/build_cultural_dimensions.R\n",
    "    \"\"\"\n",
    "    \n",
    "    diffs = []\n",
    "    \n",
    "    for pair in word_pairs:\n",
    "        \n",
    "        word1, word2 = pair\n",
    "        \n",
    "        if word1 in word_vectors and word2 in word_vectors:\n",
    "\n",
    "            vec1 = word_vectors[pair[0]]\n",
    "            vec1 = vec1/np.linalg.norm(vec1) # norm the first word vector\n",
    "\n",
    "            vec2 = word_vectors[pair[1]]\n",
    "            vec2 = vec2/np.linalg.norm(vec2) # norm the second second vector\n",
    "\n",
    "            diff = vec1 - vec2\n",
    "            diff = diff/np.linalg.norm(diff) # norm the difference between the vectors\n",
    "            diffs.append(diff)               # append the difference to the list\n",
    "            \n",
    "        else:\n",
    "            print(f\"Missing: {pair}\") # missing words from affluence pairs\n",
    "        \n",
    "    dim = np.mean(diffs, axis=0) # average the vectors appended to the list\n",
    "    dim = dim/np.linalg.norm(dim) # norm the average\n",
    "    \n",
    "    return dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the cultural dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_dim = make_dim(aff_pairs)\n",
    "gender_dim = make_dim(gender_pairs)\n",
    "race_dim = make_dim(race_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each is a 300-dimensional vector just like the word vectors, but it doesn't correspond to a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(aff_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_dim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locating musical genres along these dimensions\n",
    "\n",
    "Now we can create a plot like Figure 3 in the original paper (p. 921). We won't overlay a plot based on survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = [\"jazz\", \"rap\", \"opera\", \"punk\", \"techno\", \"hiphop\", \"bluegrass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_proj_dict = {}\n",
    "gender_proj_dict = {}\n",
    "race_proj_dict = {}\n",
    "\n",
    "for genre in genres:\n",
    "    cos_aff = cosine_reshape(word_vectors[genre], aff_dim)\n",
    "    aff_proj_dict[genre] = cos_aff\n",
    "    \n",
    "    cos_gender = cosine_reshape(word_vectors[genre], gender_dim)\n",
    "    gender_proj_dict[genre] = cos_gender\n",
    "    \n",
    "    cos_race = cosine_reshape(word_vectors[genre], race_dim)\n",
    "    race_proj_dict[genre] = cos_race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre in genres:\n",
    "    plt.text(x=race_proj_dict[genre], y=aff_proj_dict[genre], s=genre)\n",
    "    \n",
    "plt.xlim(-0.05,0.3)\n",
    "plt.ylim(-0.10, 0.15)\n",
    "\n",
    "plt.xlabel(\"Race Projection\")\n",
    "plt.ylabel(\"Class Projection\")\n",
    "plt.title(\"Class and Racial Associations of Musical Genres\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for genre in genres:\n",
    "    plt.text(x=race_proj_dict[genre], y=gender_proj_dict[genre], s=genre)\n",
    "    \n",
    "plt.xlim(-0.05,0.3)\n",
    "plt.ylim(-0.10, 0.15)\n",
    "\n",
    "plt.xlabel(\"Race Projection\")\n",
    "plt.ylabel(\"Gender Projection\")\n",
    "plt.title(\"Gender and Racial Associations of Musical Genres\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, Kozlowski et al. (2019) compare their results to survey data and find similar associations between musical genres and these projections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1</b><br><br>\n",
    "    For this exercise, you will identify another domain of social life that may exhibit the kind of bias we see above.<br><br>\n",
    "    1.1 Create a list of things associated with the domain you have identified, just like the list of musical genres above (<tt>genres</tt>). Only include words that are in the vocabulary of the word embedding model you are using.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `filter()` method and a lambda function to get rid of any words that aren't in the word embedding model. Assuming your word embedding model is named <tt>word_vectors</tt> (as above) and your list is called <tt>list_of_things</tt> (which is optional), the following line would keep only the words that are in the word embedding model's vocabulary.\n",
    "\n",
    "```python\n",
    "list_of_things = list(filter(lambda x: x in word_vectors, list_of_things))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    1.2 What domain did you choose? Why? What associations do you expect to see with the cultural dimensions we've measured?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    1.3 Replace <tt>LIST_OF_THINGS</tt> with the name of your list and run the cell below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in LIST_OF_THINGS: # replace LIST_OF_THINGS\n",
    "    cos_aff = cosine_reshape(word_vectors[element], aff_dim)\n",
    "    aff_proj_dict[element] = cos_aff\n",
    "    \n",
    "    cos_gender = cosine_reshape(word_vectors[element], gender_dim)\n",
    "    gender_proj_dict[element] = cos_gender\n",
    "    \n",
    "    cos_race = cosine_reshape(word_vectors[element], race_dim)\n",
    "    race_proj_dict[element] = cos_race"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    1.4 Now plot the positions of the words you chose in a two-dimensional space defined by two of the three cultural dimensions we've measured. (The code in the cells with the plots above should only need minor tweaks to plot your words.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    1.5 Do the results match the associations you predicted in 1.2?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2</b><br><br>\n",
    "    For this exercise, you will identify and measure a new cultural dimension.<br><br>\n",
    "    2.1 Begin by identifying pairs of words that should be at opposite ends of the dimension. These may be antonyms like \"rich\" and \"poor\" or they may be anchors for a cultural dimension related to stereotyping and bias that aren't necessarily antonyms. (For example, \"Black\" and \"White\" as ethnicities aren't antonyms, but are useful anchors for identifying a cultural dimension that allows us to measure and visualize bias.) <br><br>\n",
    "    You can create this list of word pairs as a list of lists or list of tuples inside the notebook, or you might try saving them in a CSV file and opening them with the <tt>read_pairs_from_file()</tt> function defined above. NOTE: The anchors should always be in the same order. For example, in <tt>affluence_pairs.csv</tt>, words associated with the affluent end of the spectrum are always listed first, and the antonyms are listed the second. You can look at the CSVs used above for guidance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    2.2 What is your cultural dimension meant to measure? What kinds of stereotypes might you see?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    2.3 Calculate the cultural dimension using the <tt>make_dim()</tt> function defined above. This should look like the following (but with your word pairs, and a name other than <tt>aff_dim</tt>):\n",
    "</div>\n",
    "\n",
    "```python\n",
    "aff_dim = make_dim(aff_pairs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    2.4 Now using your dimension and one of the others, plot a list of words in the two-dimensional space defined by the dimensions like in 1.4.<br><br>\n",
    "    - Rename <tt>YOUR_DIM_PROJ_DICT</tt><br>\n",
    "    - Replace <tt>LIST_OF_THINGS</tt> with the list you want to use (which could be musical genres, the list you used in Exercise 1, or something else)<br>\n",
    "    - Replace <tt>YOUR_DIM</tt> with the name of the variable to which you've assigned your cultural dimension<br>\n",
    "    - Replace <tt>YOUR_DIM_PROJ_DICT</tt> in the last line with the name you chose in the first line<br>\n",
    "    - Create a plot using your dimension and one other dimension\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_DIM_PROJ_DICT = {} # rename this\n",
    "\n",
    "for element in LIST_OF_THINGS: # replace LIST_OF_THINGS\n",
    "    cos_aff = cosine_reshape(word_vectors[element], aff_dim)\n",
    "    aff_proj_dict[element] = cos_aff\n",
    "    \n",
    "    cos_gender = cosine_reshape(word_vectors[element], gender_dim)\n",
    "    gender_proj_dict[element] = cos_gender\n",
    "    \n",
    "    cos_race = cosine_reshape(word_vectors[element], race_dim)\n",
    "    race_proj_dict[element] = cos_race\n",
    "    \n",
    "    cos_new = cosine_reshape(word_vectors[element], YOUR_DIM) # replace YOUR_DIM with the name of your cultural dim.\n",
    "    YOUR_DIM_PROJ_DICT[element] = cos_new # replace the name of the dictionary with the name you chose above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    2.5 Do the results match the associations you predicted in 2.2?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your answer here_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
