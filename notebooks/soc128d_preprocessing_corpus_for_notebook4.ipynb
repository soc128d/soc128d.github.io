{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "from empath import Empath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`re`](https://docs.python.org/3/library/re.html) is a library for using regular expressions (patterns of characters used to match more varied strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"rjobs_2020_raw.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70711, 91)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping posts that have selftext and meet other criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df[\"locked\"]==False) & (df[\"selftext\"] != \"[removed]\") & (df[\"selftext\"] != \"[deleted]\")\n",
    "       & (~df[\"selftext\"].isnull()) & (df[\"is_self\"]==True) & (df[\"is_video\"]==False) & (df[\"pinned\"]==False)\n",
    "       & (df[\"stickied\"]==False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49872, 91)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting \"created_utc\" to the date and then creating different time variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = pd.to_datetime(df[\"created_utc\"],unit=\"s\") # \"created_utc\" is in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-71a7d62a0199>:6: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df[\"week\"] = df[\"date\"].dt.week\n"
     ]
    }
   ],
   "source": [
    "df[\"dayofyear\"] = df[\"date\"].dt.dayofyear\n",
    "df[\"hour\"] = df[\"date\"].dt.hour\n",
    "df[\"dayofmonth\"] = df[\"date\"].dt.day\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df[\"dayofweek\"] = df[\"date\"].dt.dayofweek\n",
    "df[\"week\"] = df[\"date\"].dt.week\n",
    "df[\"day_name\"] = df[\"date\"].dt.day_name()\n",
    "df[\"month_name\"] = df[\"date\"].dt.month_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly shuffling the data and then creating anonymized author IDs\n",
    "\n",
    "I already anonymized these (the \"author_pseudo\" field) but this shows the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1.0) # this random \"sample\" is 100% of the data points, just in a different order\n",
    "\n",
    "authors = set(df[\"author_pseudo\"]) # set of all the unique author usernames\n",
    "\n",
    "author_ids = np.random.randint(0,1000000, len(authors)) # random numbers between 0 and 1,000,000\n",
    "author_ids = author_ids.tolist() # convert it from an array to a list\n",
    "while len(set(author_ids)) < len(authors): # because the numbers are random, some may be duplicates\n",
    "    author_ids = list(set(author_ids)) # cast it to set to nix duplicates, then back to list\n",
    "    new = np.random.randint(0,100000) # get one new random number\n",
    "    new_idx = np.random.randint(0,len(author_ids)-1) # get one random index\n",
    "    author_ids.insert(new_idx, new) # insert the new random number at the random index\n",
    "    \n",
    "# the while loop will keep going until there are the same number of unique IDs as there are unique authors\n",
    "\n",
    "author_id = [f\"{id_:0>6}\" for id_ in author_ids] # convert them to strings with leading zeros, e.g. '000001'\n",
    "author_id_dict = {author:author_id for author, author_id in zip(authors, author_ids)} # dict mapping authors to IDs\n",
    "\n",
    "df[\"author_id\"] = df[\"author_pseudo\"].apply(lambda x: author_id_dict[x]) # create new variable, anonymizing authors\n",
    "\n",
    "# we drop the author column later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(authors)) == len(set(author_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the \"text\" field by merging the title and selftext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df.apply(lambda row: row[\"title\"] + \"\\n \" + row[\"selftext\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_post(post: str) -> str:\n",
    "    \"\"\"\n",
    "    Tokenize, lemmatize, remove stop words, \n",
    "    remove non-alphabetic characters.\n",
    "    \"\"\"\n",
    "    post = \" \".join([word.lemma_ for word in nlp(post) if not word.is_stop])\n",
    "    post = re.sub(\"[^a-z]\", \" \", post.lower())\n",
    "    return re.sub(\"\\s+\", \" \", post).strip()\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should I quit my very first job after working their for less than a few days\n",
      " I've recently got a job at Joann's (fabric and craft store) and I absolutely hate my job, being a stocker - and hate the store. The store is ran-down and dirty, and everybody who I deal with are older lady's and  Karens. Im the youngest person (only 16) at the store and don't really in there, however Im paid more than minimum wage (Im paid $9.00 while minimum wage is $8.25) Do you think it will be worth it or would nobody want to hire me because I quit my very first job within a few days? any advice helps, thank you.\n"
     ]
    }
   ],
   "source": [
    "example = df.sample(1)[\"text\"].values[0]\n",
    "\n",
    "# df.sample(1) takes 1 row from the data frame at random\n",
    "# [\"text\"] selects the \"text\" field (which is the combination of the \"title\" and \"selftext\" fields created above)\n",
    "# .values turns it into a vector of strings, in this case just the one\n",
    "# [0] takes the first element in the vector (which is also the only element)\n",
    "# now we have the string for the text instead of a dataframe or some kind of vector\n",
    "\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Should | lemma: should | part of speech: AUX\n",
      "word: I | lemma: I | part of speech: PRON\n",
      "word: quit | lemma: quit | part of speech: VERB\n",
      "word: my | lemma: my | part of speech: PRON\n",
      "word: very | lemma: very | part of speech: ADV\n",
      "word: first | lemma: first | part of speech: ADJ\n",
      "word: job | lemma: job | part of speech: NOUN\n",
      "word: after | lemma: after | part of speech: ADP\n",
      "word: working | lemma: work | part of speech: VERB\n",
      "word: their | lemma: their | part of speech: PRON\n",
      "word: for | lemma: for | part of speech: ADP\n",
      "word: less | lemma: less | part of speech: ADJ\n",
      "word: than | lemma: than | part of speech: SCONJ\n",
      "word: a | lemma: a | part of speech: DET\n",
      "word: few | lemma: few | part of speech: ADJ\n",
      "word: days | lemma: day | part of speech: NOUN\n",
      "word: \n",
      "  | lemma: \n",
      "  | part of speech: SPACE\n",
      "word: I | lemma: I | part of speech: PRON\n",
      "word: 've | lemma: 've | part of speech: AUX\n",
      "word: recently | lemma: recently | part of speech: ADV\n",
      "word: got | lemma: get | part of speech: VERB\n",
      "word: a | lemma: a | part of speech: DET\n",
      "word: job | lemma: job | part of speech: NOUN\n",
      "word: at | lemma: at | part of speech: ADP\n",
      "word: Joann | lemma: Joann | part of speech: PROPN\n",
      "word: 's | lemma: 's | part of speech: PART\n",
      "word: ( | lemma: ( | part of speech: PUNCT\n",
      "word: fabric | lemma: fabric | part of speech: NOUN\n",
      "word: and | lemma: and | part of speech: CCONJ\n",
      "word: craft | lemma: craft | part of speech: NOUN\n",
      "word: store | lemma: store | part of speech: NOUN\n",
      "word: ) | lemma: ) | part of speech: PUNCT\n",
      "word: and | lemma: and | part of speech: CCONJ\n",
      "word: I | lemma: I | part of speech: PRON\n",
      "word: absolutely | lemma: absolutely | part of speech: ADV\n",
      "word: hate | lemma: hate | part of speech: VERB\n",
      "word: my | lemma: my | part of speech: PRON\n",
      "word: job | lemma: job | part of speech: NOUN\n",
      "word: , | lemma: , | part of speech: PUNCT\n",
      "word: being | lemma: be | part of speech: VERB\n",
      "word: a | lemma: a | part of speech: DET\n",
      "word: stocker | lemma: stocker | part of speech: NOUN\n",
      "word: - | lemma: - | part of speech: PUNCT\n",
      "word: and | lemma: and | part of speech: CCONJ\n",
      "word: hate | lemma: hate | part of speech: VERB\n",
      "word: the | lemma: the | part of speech: DET\n",
      "word: store | lemma: store | part of speech: NOUN\n",
      "word: . | lemma: . | part of speech: PUNCT\n",
      "word: The | lemma: the | part of speech: DET\n",
      "word: store | lemma: store | part of speech: NOUN\n",
      "word: is | lemma: be | part of speech: AUX\n",
      "word: ran | lemma: run | part of speech: VERB\n",
      "word: - | lemma: - | part of speech: PUNCT\n",
      "word: down | lemma: down | part of speech: NOUN\n",
      "word: and | lemma: and | part of speech: CCONJ\n",
      "word: dirty | lemma: dirty | part of speech: ADJ\n",
      "word: , | lemma: , | part of speech: PUNCT\n",
      "word: and | lemma: and | part of speech: CCONJ\n",
      "word: everybody | lemma: everybody | part of speech: PRON\n",
      "word: who | lemma: who | part of speech: PRON\n",
      "word: I | lemma: I | part of speech: PRON\n",
      "word: deal | lemma: deal | part of speech: VERB\n",
      "word: with | lemma: with | part of speech: ADP\n",
      "word: are | lemma: be | part of speech: VERB\n",
      "word: older | lemma: old | part of speech: ADJ\n",
      "word: lady | lemma: lady | part of speech: NOUN\n",
      "word: 's | lemma: 's | part of speech: PART\n",
      "word: and | lemma: and | part of speech: CCONJ\n",
      "word:   | lemma:   | part of speech: SPACE\n",
      "word: Karens | lemma: Karens | part of speech: PROPN\n",
      "word: . | lemma: . | part of speech: PUNCT\n",
      "word: I | lemma: I | part of speech: PRON\n",
      "word: m | lemma: m | part of speech: VERB\n",
      "word: the | lemma: the | part of speech: DET\n",
      "word: youngest | lemma: young | part of speech: ADJ\n",
      "word: person | lemma: person | part of speech: NOUN\n",
      "word: ( | lemma: ( | part of speech: PUNCT\n",
      "word: only | lemma: only | part of speech: ADV\n",
      "word: 16 | lemma: 16 | part of speech: NUM\n",
      "word: ) | lemma: ) | part of speech: PUNCT\n",
      "word: at | lemma: at | part of speech: ADP\n",
      "word: the | lemma: the | part of speech: DET\n",
      "word: store | lemma: store | part of speech: NOUN\n",
      "word: and | lemma: and | part of speech: CCONJ\n",
      "word: do | lemma: do | part of speech: VERB\n",
      "word: n't | lemma: n't | part of speech: PART\n",
      "word: really | lemma: really | part of speech: ADV\n",
      "word: in | lemma: in | part of speech: ADV\n",
      "word: there | lemma: there | part of speech: ADV\n",
      "word: , | lemma: , | part of speech: PUNCT\n",
      "word: however | lemma: however | part of speech: ADV\n",
      "word: I | lemma: I | part of speech: PRON\n",
      "word: m | lemma: m | part of speech: AUX\n",
      "word: paid | lemma: pay | part of speech: VERB\n",
      "word: more | lemma: more | part of speech: ADJ\n",
      "word: than | lemma: than | part of speech: SCONJ\n",
      "word: minimum | lemma: minimum | part of speech: ADJ\n",
      "word: wage | lemma: wage | part of speech: NOUN\n",
      "word: ( | lemma: ( | part of speech: PUNCT\n",
      "word: I | lemma: I | part of speech: PRON\n",
      "word: m | lemma: m | part of speech: AUX\n",
      "word: paid | lemma: pay | part of speech: VERB\n",
      "word: $ | lemma: $ | part of speech: SYM\n",
      "word: 9.00 | lemma: 9.00 | part of speech: NUM\n",
      "word: while | lemma: while | part of speech: SCONJ\n",
      "word: minimum | lemma: minimum | part of speech: ADJ\n",
      "word: wage | lemma: wage | part of speech: NOUN\n",
      "word: is | lemma: be | part of speech: VERB\n",
      "word: $ | lemma: $ | part of speech: SYM\n",
      "word: 8.25 | lemma: 8.25 | part of speech: NUM\n",
      "word: ) | lemma: ) | part of speech: PUNCT\n",
      "word: Do | lemma: do | part of speech: AUX\n",
      "word: you | lemma: you | part of speech: PRON\n",
      "word: think | lemma: think | part of speech: VERB\n",
      "word: it | lemma: it | part of speech: PRON\n",
      "word: will | lemma: will | part of speech: AUX\n",
      "word: be | lemma: be | part of speech: VERB\n",
      "word: worth | lemma: worth | part of speech: ADJ\n",
      "word: it | lemma: it | part of speech: PRON\n",
      "word: or | lemma: or | part of speech: CCONJ\n",
      "word: would | lemma: would | part of speech: AUX\n",
      "word: nobody | lemma: nobody | part of speech: PRON\n",
      "word: want | lemma: want | part of speech: VERB\n",
      "word: to | lemma: to | part of speech: PART\n",
      "word: hire | lemma: hire | part of speech: VERB\n",
      "word: me | lemma: I | part of speech: PRON\n",
      "word: because | lemma: because | part of speech: SCONJ\n",
      "word: I | lemma: I | part of speech: PRON\n",
      "word: quit | lemma: quit | part of speech: VERB\n",
      "word: my | lemma: my | part of speech: PRON\n",
      "word: very | lemma: very | part of speech: ADV\n",
      "word: first | lemma: first | part of speech: ADJ\n",
      "word: job | lemma: job | part of speech: NOUN\n",
      "word: within | lemma: within | part of speech: ADP\n",
      "word: a | lemma: a | part of speech: DET\n",
      "word: few | lemma: few | part of speech: ADJ\n",
      "word: days | lemma: day | part of speech: NOUN\n",
      "word: ? | lemma: ? | part of speech: PUNCT\n",
      "word: any | lemma: any | part of speech: DET\n",
      "word: advice | lemma: advice | part of speech: NOUN\n",
      "word: helps | lemma: help | part of speech: VERB\n",
      "word: , | lemma: , | part of speech: PUNCT\n",
      "word: thank | lemma: thank | part of speech: VERB\n",
      "word: you | lemma: you | part of speech: PRON\n",
      "word: . | lemma: . | part of speech: PUNCT\n"
     ]
    }
   ],
   "source": [
    "for word in nlp(example):\n",
    "    print(f\"word: {word.text} | lemma: {word.lemma_} | part of speech: {word.pos_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quit job work day recently get job joann fabric craft store absolutely hate job stocker hate store store run dirty everybody deal old lady karens m young person store m pay minimum wage m pay minimum wage think worth want hire quit job day advice help thank\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_post(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"<tt>nlp</tt>\" is a language model from [spaCy](https://spacy.io/). It does part-of-speech tagging, named entity recognition, and more. `disable=[\"ner\"]` tells it not to perform named entity recognition. Turning things off might speed it up\n",
    "\n",
    "The function <tt>preprocess_post</tt> is equivalent to the following:\n",
    "\n",
    "```python\n",
    "def preprocess_post(post: str) -> str:\n",
    "    \"\"\"\n",
    "    Tokenizes and returns the lowercase lemmas of\n",
    "    tokens that are not stop words, minus any \n",
    "    non-alphabetic characters\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    for word in nlp(post): # each \"word\" in nlp(post) has been part-of-speech tagged, etc.\n",
    "        if not word.is_stop: # \".is_stop\" checks whether spacy has determined it's a stop word\n",
    "            words.append(word.lemma_) # adding the lemma of the word, not the word itself, to the list\n",
    "    post = \" \".join(words) # converting the list of words to a string variable separated by spaces\n",
    "    post = post.lower() # make everything lowercase\n",
    "    post = re.sub(\"[^a-z]\", \" \", post) # now we replace non-alphabetic chars with spaces\n",
    "    post = re.sub(\"\\s+\", \" \", post) # now we replace long stretches of whitespace with a single space\n",
    "    post = post.strip() # now we strip whitespace from the edges\n",
    "    return post\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing 49872 posts in 12.6 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df[\"preprocessed\"] = df[\"text\"].apply(preprocess_post)\n",
    "\n",
    "print(f\"Finished preprocessing {df.shape[0]} posts in {(time.time()-start_time)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This took about twice as long in Windows, for what that's worth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating scores for each dictionary from Empath for each post in corpus\n",
    "\n",
    "This calls lexicon.analyze() on each preprocessed post. lexicon.analyze() returns a dictionary with lexical categories as keys and a post's score as the value for each. This creates a column (variable) for each key and populates it with each post's score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-fc255edf12e4>:5: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df[list(lexicon.cats)] = df[\"preprocessed\"].apply(lambda x: pd.Series(lexicon.analyze(x, normalize=True)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzed all posts in 7.1 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "lexicon = Empath()\n",
    "\n",
    "df[list(lexicon.cats)] = df[\"preprocessed\"].apply(lambda x: pd.Series(lexicon.analyze(x, normalize=True)))\n",
    "\n",
    "print(f\"Analyzed all posts in {(time.time()-start_time)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of Googling suggests .apply() is slow. Other methods also give warnings, but I'll look into alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it!\n",
    "\n",
    "Now we just get the subset of columns we want and export the whole dataframe to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"id\"] = [f\"{i:0>5}\" for i in range(df.shape[0])] # create an ID as a string, based on order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"id\", \"author_id\", \"score\", \"num_comments\", \"title\", \"selftext\", \"text\", \"preprocessed\", \"date\", \"dayofyear\", \"hour\", \"dayofmonth\", \"month\", \"dayofweek\", \"week\", \"day_name\", \"month_name\"]\n",
    "cols += sorted(list(lexicon.cats.keys())) # we add the categories from Empath so we keep the columns from that, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>date</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>...</th>\n",
       "      <th>wealthy</th>\n",
       "      <th>weapon</th>\n",
       "      <th>weather</th>\n",
       "      <th>wedding</th>\n",
       "      <th>white_collar_job</th>\n",
       "      <th>work</th>\n",
       "      <th>worship</th>\n",
       "      <th>writing</th>\n",
       "      <th>youth</th>\n",
       "      <th>zest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44252</th>\n",
       "      <td>00000</td>\n",
       "      <td>651809</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>how am i supposed to find \"new\" ways of doing ...</td>\n",
       "      <td>so, i just graduated and I got the role as an ...</td>\n",
       "      <td>how am i supposed to find \"new\" ways of doing ...</td>\n",
       "      <td>suppose find new way thing graduate get role o...</td>\n",
       "      <td>2020-08-02 15:12:52</td>\n",
       "      <td>215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55101</th>\n",
       "      <td>00001</td>\n",
       "      <td>939708</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>Everything is horrible right now</td>\n",
       "      <td>I feel horrible. I feel like I should have nev...</td>\n",
       "      <td>Everything is horrible right now\\n I feel horr...</td>\n",
       "      <td>horrible right feel horrible feel like get bac...</td>\n",
       "      <td>2020-09-29 01:02:56</td>\n",
       "      <td>273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62790</th>\n",
       "      <td>00002</td>\n",
       "      <td>368221</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>WGAT SHOULD I DO</td>\n",
       "      <td>I work with 2 girls,    that are younger than ...</td>\n",
       "      <td>WGAT SHOULD I DO\\n I work with 2 girls,    tha...</td>\n",
       "      <td>wgat work girl young clean roof gutter story h...</td>\n",
       "      <td>2020-11-12 20:27:03</td>\n",
       "      <td>317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8254</th>\n",
       "      <td>00003</td>\n",
       "      <td>543188</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>I really want this job and today they will con...</td>\n",
       "      <td>What can background checks find?\\n\\nRecently g...</td>\n",
       "      <td>I really want this job and today they will con...</td>\n",
       "      <td>want job today confirm question background che...</td>\n",
       "      <td>2020-02-07 15:58:03</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070175</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23535</th>\n",
       "      <td>00004</td>\n",
       "      <td>330821</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Help big time</td>\n",
       "      <td>Okay guys just did a interview online via vide...</td>\n",
       "      <td>Help big time\\n Okay guys just did a interview...</td>\n",
       "      <td>help big time okay guy interview online video ...</td>\n",
       "      <td>2020-04-25 10:37:05</td>\n",
       "      <td>116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 214 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  author_id  score  num_comments  \\\n",
       "44252  00000     651809      1             3   \n",
       "55101  00001     939708      1            17   \n",
       "62790  00002     368221      1             5   \n",
       "8254   00003     543188      1             5   \n",
       "23535  00004     330821      1             1   \n",
       "\n",
       "                                                   title  \\\n",
       "44252  how am i supposed to find \"new\" ways of doing ...   \n",
       "55101                   Everything is horrible right now   \n",
       "62790                                   WGAT SHOULD I DO   \n",
       "8254   I really want this job and today they will con...   \n",
       "23535                                      Help big time   \n",
       "\n",
       "                                                selftext  \\\n",
       "44252  so, i just graduated and I got the role as an ...   \n",
       "55101  I feel horrible. I feel like I should have nev...   \n",
       "62790  I work with 2 girls,    that are younger than ...   \n",
       "8254   What can background checks find?\\n\\nRecently g...   \n",
       "23535  Okay guys just did a interview online via vide...   \n",
       "\n",
       "                                                    text  \\\n",
       "44252  how am i supposed to find \"new\" ways of doing ...   \n",
       "55101  Everything is horrible right now\\n I feel horr...   \n",
       "62790  WGAT SHOULD I DO\\n I work with 2 girls,    tha...   \n",
       "8254   I really want this job and today they will con...   \n",
       "23535  Help big time\\n Okay guys just did a interview...   \n",
       "\n",
       "                                            preprocessed                date  \\\n",
       "44252  suppose find new way thing graduate get role o... 2020-08-02 15:12:52   \n",
       "55101  horrible right feel horrible feel like get bac... 2020-09-29 01:02:56   \n",
       "62790  wgat work girl young clean roof gutter story h... 2020-11-12 20:27:03   \n",
       "8254   want job today confirm question background che... 2020-02-07 15:58:03   \n",
       "23535  help big time okay guy interview online video ... 2020-04-25 10:37:05   \n",
       "\n",
       "       dayofyear  ...   wealthy    weapon   weather   wedding  \\\n",
       "44252        215  ...  0.025000  0.000000  0.000000  0.000000   \n",
       "55101        273  ...  0.029412  0.000000  0.014706  0.014706   \n",
       "62790        317  ...  0.030303  0.000000  0.000000  0.000000   \n",
       "8254          38  ...  0.000000  0.017544  0.000000  0.000000   \n",
       "23535        116  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       white_collar_job      work worship   writing     youth  zest  \n",
       "44252          0.025000  0.025000     0.0  0.000000  0.000000   0.0  \n",
       "55101          0.073529  0.147059     0.0  0.014706  0.000000   0.0  \n",
       "62790          0.000000  0.060606     0.0  0.000000  0.060606   0.0  \n",
       "8254           0.070175  0.105263     0.0  0.000000  0.017544   0.0  \n",
       "23535          0.040000  0.120000     0.0  0.040000  0.000000   0.0  \n",
       "\n",
       "[5 rows x 214 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"rjobs_2020_cleaned.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
