{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sociology 128D: Mining Culture Through Text Data: Introduction to Social Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 9: Training Word Embeddings using `gensim`\n",
    "\n",
    "In this notebook, we'll explore implementing the popular word2vec algorithm for training word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the data from Canvas (<tt>Files -> Data</tt>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"rjobs_2020_preprocessed.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are training a word embedding model, we will only use the text. Any analyses we do will be based on relationships among words (well, their embeddings), so we won't use document-level metadata like the date or number of replies.\n",
    "\n",
    "The line of code below does three things:\n",
    "1. `.apply(str.split)` splits the preprocessed text on whitespace\n",
    "2. `.tolist()` converts the column to a list\n",
    "3. Finally, we assign the result--a list of lists--to the variable <tt>text</tt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df[\"preprocessed\"].apply(str.split).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will learn representations of words (as vectors) based on how the words are used. It will learn less about rare words. Below we can see the impact on the vocabulary size of excluding words that don't appear a minimum of five or 25 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(filter(lambda x: x[1] >= 5, Counter([word for post in text for word in post]).items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(filter(lambda x: x[1] >= 25, Counter([word for post in text for word in post]).items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for post in text for word in post]\n",
    "min_of_five = [word for word, count in Counter(all_words).items() if count >= 5]\n",
    "min_of_twentyfive = [word for word, count in Counter(all_words).items() if count >= 25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we exclude words that do not appear a minimum of 25 times, we lose words like \"cheesecake\" that actually seem quite relevant to the example below. Whether or not we keep a word like \"cheesecake\" matters for three reasons. First, if we exclude it, we do not get a word vector for it, and we cannot use it for any analyses. Second, representations for words like \"factory\" will be affect because they directly co-occur. Finally, if we remove words, we also bring the remaining words closer together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = df.loc[5239].preprocessed\n",
    "\n",
    "samp_min_five = \" \".join([word for word in samp.split() if word in min_of_five])\n",
    "samp_min_twentyfive = \" \".join([word for word in samp.split() if word in min_of_twentyfive])\n",
    "\n",
    "print(samp_min_five, \"\\n\")\n",
    "print(samp_min_twentyfive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can speed things up by changing the number of workers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below is adapted from https://stackoverflow.com/a/58515344. It allows us to print the loss when we train the model for more than one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch.\n",
    "    from https://stackoverflow.com/a/58515344\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_to_be_subed = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subed\n",
    "        self.loss_to_be_subed = loss\n",
    "        print(f\"Loss after epoch {self.epoch}: {loss_now:,}\")\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Basic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the details of `gensim`'s implementation of word2vec [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec). The <tt>sg</tt> argument let's use the skip-gram algorithm, and <tt>negative</tt> let's use specify the number of negative samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = Word2Vec(text, window = 5, sg = 1, negative = 5, workers = os.cpu_count()-1, min_count = 5)\n",
    "basic_model = basic_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model.most_similar(\"employment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model.most_similar(\"job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Better Model\n",
    "\n",
    "Now let's compare that model to a better model. We're going to train a model several epochs, meaning the model will have several chances to update the word embeddings and improve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model = Word2Vec(text, vector_size = 300, window = 7, sg = 1, negative = 5, workers = os.cpu_count()-1, min_count = 5, \n",
    "                 epochs = 100, callbacks=[callback()], compute_loss = True)\n",
    "\n",
    "minutes = (time.time() - start_time)/60\n",
    "print(f\"Training completed in {minutes:.1f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"employment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"job\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
