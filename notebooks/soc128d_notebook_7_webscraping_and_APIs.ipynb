{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sociology 128D: Mining Culture Through Text Data: Introduction to Social Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 7: Web Scraping and APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is a big topic. There are a lot of reasons someone might want to scrape web content, but the reason applicable to this class is to get data that may be useful for answering questions about some social phenomena.\n",
    "\n",
    "People who provide web content are typically savvy to the existence of tools for web scraping. You can often find references to automated web scraping in a site's Terms of Use (or equivalent), which often prohibit automated scraping.\n",
    "\n",
    "I'll just make two points here. First, the desirability of the data on a site is probably positively correlated with how prohibitive it is to scrape it. Second, we should try to be clear about what we mean by \"web scraping.\"\n",
    "\n",
    "Regarding the second point, we are typically referring to accessing a website's content in a way that's mediated by a tool or set of tools that makes it qualitatively different from browsing the web normally. As we'll see in our first example using the `requests` library, this can be as simple as using a line of Python code to store a web search in memory, rather than rendering it directly in a browser. We can then view what we've scraped (e.g., rendered HTML), which wouldn't be much different from normal browsing. We could also save it, or save some feature or set of features we've extracted from it; and doing this a lot is typically where things become problematic.\n",
    "\n",
    "At the most basic level, repeatedly scraping a site (or some part of it) means making repeated requests of the site's servers. That can be a problem in itself. The first point above just adds to this: sites may also want to protect their data, and may make it available subject to terms that prohibit automated scraping. Content is also served in different ways. Static websites are much easier to scrape than dynamic ones, which require a different approach.\n",
    "\n",
    "One compromise many sites make is to offer an application programming interface (API). In this notebook, we're going to keep our focus on getting data that may be useful for answering social research questions. Toward that end, we'll explore scraping static web content with an eye toward getting Twitter user handles for members of the US senate, and we'll then use those handles to get tweets. Finally, we'll use an API to access data from Reddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For this notebook, you'll need to install `beautifulsoup4`, `psaw`, `nest_asyncio`, and `twint`.\n",
    "\n",
    "If you use Anaconda, you can install `beautifulsoup4` and `async_io` by running the following lines in the Anaconda interpreter:\n",
    "\n",
    "```\n",
    "conda install -c anaconda beautifulsoup4 \n",
    "conda install -c conda-forge nest-asyncio\n",
    "```\n",
    "\n",
    "Otherwise, you install them using pip. (Depending on your setup, you may need to use pip3 instead.)\n",
    "\n",
    "```\n",
    "pip3 install beautifulsoup4\n",
    "pip3 install nest_asyncio\n",
    "```\n",
    "\n",
    "Regardless, you will need to install `psaw` using pip:\n",
    "\n",
    "```\n",
    "pip3 install psaw\n",
    "```\n",
    "\n",
    "And you will need to install `twint` by executing the following commands from the command line (e.g., the Anaconda interpreter):\n",
    "\n",
    "```\n",
    "git clone --depth=1 https://github.com/twintproject/twint.git\n",
    "cd twint\n",
    "pip3 install . -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import twint\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import display, HTML\n",
    "from psaw import PushshiftAPI\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping with Requests and BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1. Rendering Search Results inside Jupyter\n",
    "\n",
    "At its most basic level, \"scraping the web\" is just using a computer to access web content in a different way. The next two cells show how we can use the `requests` library to store the results of a web search in memory (in a variable we'll call <tt>results</tt>), which we can then render inside the notebook.\n",
    "\n",
    "We'll use `requests.get()` to get the web content we want to examine. The [`requests` library](https://docs.python-requests.org/en/master/) enables us to make HTTP requests, even with authentication.\n",
    "\n",
    "Running the second cell may change the way the notebook is displayed. You can comment it out and run the cell again if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.google.com/search?q=weather+stanford\"\n",
    "results = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(HTML(results.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2. Scraping Quotes from a Scraping Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of how scraping static content works, we'll start with a sandbox designed for this purpose. https://toscrape.com/ offers a couple of environments, including a [fictional bookstore](https://books.toscrape.com/). Since this is a class on text analysis, we're going to take a look at [another page](https://quotes.toscrape.com/), which displays quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://quotes.toscrape.com/\"\n",
    "quotes_page = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_page.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that we can interact with the result like it's a string. If you type \"quotes_page.\" (ending with a period) and press the `tab` key, Jupyter will list several attributes you can explore, like the status code and headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quotes_page.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_page.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_page.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to parse the text and find the content we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(quotes_page.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now search the <tt>soup</tt> for all kinds of content. If you type \"soup.\" (ending with a period) in a Code cell and press the `tab` key, Jupyter will show different attributes or methods that are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we print one `div` section (a chunk of the HTML) that shows a single quote and the author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(soup.prettify()[600:1538])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.find_all()` method can be used for various types of content. Here we use it to get all of the `div` tags containing quotes. We then use `.find_all()` on each result to find the `span` tags nested inside. We use Python's `str.replace()` method to get rid of some unwanted text and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thing1 in soup.find_all(class_=\"quote\"):\n",
    "    for span in thing1.find_all(\"span\"):\n",
    "        print(span.text.replace(\"(about)\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3. Something Useful: Identifying Twitter Handles of Members of the Senate\n",
    "\n",
    "As we've noted, at its most basic level scraping is just accessing a site. Here we will scrape a \"real\" website--but we are only going to make *one* request. Specifically, we'll get the Twitter handles (along with state and party) of each current US senator from a site maintained by the UC San Diego Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ucsd.libguides.com/congress_twitter/senators\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senate_page = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(senate_page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(senate_page.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compare the way the HTML is printed when using `.prettify()` on <tt>soup</tt> to printing the text from the original result from `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you explore the site in a browser or just scroll through the <tt>soup</tt>, you can see that the names, states, parties, and Twitter handles of the senators are arranged in a table, which is convenient for us. We'll use `.find_all()` to identify the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all(\"table\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = soup.find_all(\"table\")\n",
    "for table in tables:\n",
    "    print(type(table), len(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that the info we want is inside `tr` tags, which are rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(tables[0])[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information we want for each senator (name, handle, state, and party) is contained in one row. The handle is in the URL of the `a` tag, while the senator's name is in the text of that tag. The state and party are in additional `td` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0].findAll(\"tr\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use `enumerate()` with a for loop just to look at the first few results.\n",
    "\n",
    "This code finds all of the `tr` tags, ignores any without a link (e.g., to a Twitter account), finds all of the elements of the `ck_border` class, and prints the text. This prints the senator's name, state, and party. The `a` tag's attributes are like a dictionary, and the value for the key \"href\" is the URL to the senator's Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, result in enumerate(soup.find_all(\"tr\")):\n",
    "    if i < 4:\n",
    "        if result.a:\n",
    "            for element in result.find_all(class_=\"ck_border\"):\n",
    "                print(element.text)\n",
    "            print(result.a.attrs[\"href\"])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have figured out the way the information is structured, we will extract the name, state, party, and Twitter handle for each US senator. We'll create an empty list called <tt>senator_data</tt> to store the data initially. We'll use a nested for loop just like the one above, for we'll append each senator's name, state, party, and handle to a list called <tt>row</tt> before appending that row--one per senator--to <tt>senator_data</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senator_data = []\n",
    "\n",
    "for result in soup.find_all(\"tr\"):\n",
    "    if result.a:\n",
    "        row = []\n",
    "        for element in result.find_all(class_=\"ck_border\"):\n",
    "            row.append(element.text)\n",
    "        handle = result.a.attrs[\"href\"]\n",
    "        handle = handle.replace(\"https://twitter.com/\", \"\")\n",
    "        row.append(handle)\n",
    "        senator_data.append(row)\n",
    "    else:\n",
    "        print(result) # show the rows that aren't added to the dataset we're making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senator_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(senator_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a pandas dataframe from this list of lists. The `columns` argument lets us name the columns in the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(senator_data, columns=[\"senator\", \"state\", \"party\", \"twitter_handle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"senate_twitter_dataframe.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Tweets using `twint`\n",
    "\n",
    "[`twint`](https://github.com/twintproject/twint) describes itself as \"an advanced Twitter scraping tool written in Python that allows for scraping Tweets from Twitter profiles without using Twitter's API.\" `twint` has been featured in plenty of guides to scraping tweets, but there seem to be issues such as the way it handles dates, among other problems. One workaround is to handle some of the configuration in the search string itself using Twitter's search operators, rather than configuring `twint` as intended.\n",
    "\n",
    "You can see Twitter's standard search operators [here](https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators).\n",
    "\n",
    "[Here are some helpful thoughts](https://thoughtfaucet.com/search-twitter-by-location/) about using (and the limitations of) location data, including [tips for finding geocodes](https://thoughtfaucet.com/search-twitter-by-location/make-a-geocode-for-twitter-location-search/) and some examples of searching for tweets from [particular events](https://thoughtfaucet.com/search-twitter-by-location/examples/).\n",
    "\n",
    "**Note:** I recommend [applying for a Twitter developer account](https://developer.twitter.com/en/apply-for-access) and accessing tweets through the official API. We will use `twint` for this example, but I do not recommend violating Twitter's terms by accessing excessive amounts of data (etc.). I've set the tweet limits low for this notebook for a reason.\n",
    "\n",
    "First, we'll look at tweets from US senators around April 28, when President Biden [addressed a joint session of Congress](https://en.wikipedia.org/wiki/2021_Joe_Biden_speech_to_a_joint_session_of_Congress). Next, we'll look at geotagged tweets.\n",
    "\n",
    "\n",
    "\n",
    "### Example 1. Tweets from US Senators\n",
    "\n",
    "We'll use the dataframe we created in the previous section to identify the twitter handles of current US senators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"senate_twitter_dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = twint.Config()\n",
    "c.Hide_output = True\n",
    "c.Store_csv = True\n",
    "c.Output = \"senate_tweets.csv\"\n",
    "c.Limit = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrape twitter data? y\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n",
      "[!] No more data! Scraping will stop now.\n",
      "found 0 deleted tweets in this search.\n"
     ]
    }
   ],
   "source": [
    "run_twint = input(\"Scrape twitter data? \")\n",
    "\n",
    "if run_twint in [\"yes\", \"y\"]:\n",
    "    for handle in df.twitter_handle.values:\n",
    "        searchstr = f\"from:{handle} until:2021-04-29 since:2021-04-28\"\n",
    "        c.Search = searchstr\n",
    "        twint.run.Search(c)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv(\"senate_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2021-04-27', '2021-04-28', (1127, 36))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.date.min(), tweets_df.date.max(), tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>timezone</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>place</th>\n",
       "      <th>...</th>\n",
       "      <th>geo</th>\n",
       "      <th>source</th>\n",
       "      <th>user_rt_id</th>\n",
       "      <th>user_rt</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>retweet_date</th>\n",
       "      <th>translate</th>\n",
       "      <th>trans_src</th>\n",
       "      <th>trans_dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1387480099763757056</td>\n",
       "      <td>1387480099763757056</td>\n",
       "      <td>2021-04-28 11:53:28 Pacific Daylight Time</td>\n",
       "      <td>2021-04-28</td>\n",
       "      <td>11:53:28</td>\n",
       "      <td>-700</td>\n",
       "      <td>1074518754</td>\n",
       "      <td>senatorbaldwin</td>\n",
       "      <td>Sen. Tammy Baldwin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1387458980025446406</td>\n",
       "      <td>1387458980025446406</td>\n",
       "      <td>2021-04-28 10:29:33 Pacific Daylight Time</td>\n",
       "      <td>2021-04-28</td>\n",
       "      <td>10:29:33</td>\n",
       "      <td>-700</td>\n",
       "      <td>1074518754</td>\n",
       "      <td>senatorbaldwin</td>\n",
       "      <td>Sen. Tammy Baldwin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1387443098502975490</td>\n",
       "      <td>1387443098502975490</td>\n",
       "      <td>2021-04-28 09:26:26 Pacific Daylight Time</td>\n",
       "      <td>2021-04-28</td>\n",
       "      <td>09:26:26</td>\n",
       "      <td>-700</td>\n",
       "      <td>1074518754</td>\n",
       "      <td>senatorbaldwin</td>\n",
       "      <td>Sen. Tammy Baldwin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1387524261951295490</td>\n",
       "      <td>1387524261951295490</td>\n",
       "      <td>2021-04-28 14:48:57 Pacific Daylight Time</td>\n",
       "      <td>2021-04-28</td>\n",
       "      <td>14:48:57</td>\n",
       "      <td>-700</td>\n",
       "      <td>202206694</td>\n",
       "      <td>senjohnbarrasso</td>\n",
       "      <td>Sen. John Barrasso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1387508940645228546</td>\n",
       "      <td>1387508940645228546</td>\n",
       "      <td>2021-04-28 13:48:04 Pacific Daylight Time</td>\n",
       "      <td>2021-04-28</td>\n",
       "      <td>13:48:04</td>\n",
       "      <td>-700</td>\n",
       "      <td>202206694</td>\n",
       "      <td>senjohnbarrasso</td>\n",
       "      <td>Sen. John Barrasso</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id      conversation_id  \\\n",
       "0  1387480099763757056  1387480099763757056   \n",
       "1  1387458980025446406  1387458980025446406   \n",
       "2  1387443098502975490  1387443098502975490   \n",
       "3  1387524261951295490  1387524261951295490   \n",
       "4  1387508940645228546  1387508940645228546   \n",
       "\n",
       "                                  created_at        date      time  timezone  \\\n",
       "0  2021-04-28 11:53:28 Pacific Daylight Time  2021-04-28  11:53:28      -700   \n",
       "1  2021-04-28 10:29:33 Pacific Daylight Time  2021-04-28  10:29:33      -700   \n",
       "2  2021-04-28 09:26:26 Pacific Daylight Time  2021-04-28  09:26:26      -700   \n",
       "3  2021-04-28 14:48:57 Pacific Daylight Time  2021-04-28  14:48:57      -700   \n",
       "4  2021-04-28 13:48:04 Pacific Daylight Time  2021-04-28  13:48:04      -700   \n",
       "\n",
       "      user_id         username                name  place  ... geo source  \\\n",
       "0  1074518754   senatorbaldwin  Sen. Tammy Baldwin    NaN  ... NaN    NaN   \n",
       "1  1074518754   senatorbaldwin  Sen. Tammy Baldwin    NaN  ... NaN    NaN   \n",
       "2  1074518754   senatorbaldwin  Sen. Tammy Baldwin    NaN  ... NaN    NaN   \n",
       "3   202206694  senjohnbarrasso  Sen. John Barrasso    NaN  ... NaN    NaN   \n",
       "4   202206694  senjohnbarrasso  Sen. John Barrasso    NaN  ... NaN    NaN   \n",
       "\n",
       "  user_rt_id user_rt retweet_id  reply_to  retweet_date  translate trans_src  \\\n",
       "0        NaN     NaN        NaN        []           NaN        NaN       NaN   \n",
       "1        NaN     NaN        NaN        []           NaN        NaN       NaN   \n",
       "2        NaN     NaN        NaN        []           NaN        NaN       NaN   \n",
       "3        NaN     NaN        NaN        []           NaN        NaN       NaN   \n",
       "4        NaN     NaN        NaN        []           NaN        NaN       NaN   \n",
       "\n",
       "  trans_dest  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>tweet</th>\n",
       "      <th>likes_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>senjohnkennedy</td>\n",
       "      <td>John Kennedy</td>\n",
       "      <td>What I expect Pres. Biden to say tonight:  1. ...</td>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>pattymurray</td>\n",
       "      <td>Senator Patty Murray</td>\n",
       "      <td>This pandemic has made clear what working moms...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>senstabenow</td>\n",
       "      <td>Sen. Debbie Stabenow</td>\n",
       "      <td>Along the Tittabawassee River, farmers will pl...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>senbobcasey</td>\n",
       "      <td>Senator Bob Casey</td>\n",
       "      <td>.@PennLive Ed. Board: \"Democrats seem confiden...</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>senfeinstein</td>\n",
       "      <td>Senator Dianne Feinstein</td>\n",
       "      <td>Reports that President Biden will increase IRS...</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>senmarkkelly</td>\n",
       "      <td>Senator Mark Kelly</td>\n",
       "      <td>For the guy who flew Neil Armstrong and Buzz A...</td>\n",
       "      <td>1813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>stevedaines</td>\n",
       "      <td>Steve Daines</td>\n",
       "      <td>The government doesn’t have a revenue problem,...</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>sentomcotton</td>\n",
       "      <td>Tom Cotton</td>\n",
       "      <td>President Biden’s tax plan would put family fa...</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>senmarkey</td>\n",
       "      <td>Ed Markey</td>\n",
       "      <td>Congratulations to my friend @SamanthaJPower o...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>senfeinstein</td>\n",
       "      <td>Senator Dianne Feinstein</td>\n",
       "      <td>Reports that the Biden administration will ban...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            username                      name  \\\n",
       "546   senjohnkennedy              John Kennedy   \n",
       "1002     pattymurray      Senator Patty Murray   \n",
       "697      senstabenow      Sen. Debbie Stabenow   \n",
       "804      senbobcasey         Senator Bob Casey   \n",
       "494     senfeinstein  Senator Dianne Feinstein   \n",
       "924     senmarkkelly        Senator Mark Kelly   \n",
       "466      stevedaines              Steve Daines   \n",
       "450     sentomcotton                Tom Cotton   \n",
       "959        senmarkey                 Ed Markey   \n",
       "113     senfeinstein  Senator Dianne Feinstein   \n",
       "\n",
       "                                                  tweet  likes_count  \n",
       "546   What I expect Pres. Biden to say tonight:  1. ...          695  \n",
       "1002  This pandemic has made clear what working moms...           77  \n",
       "697   Along the Tittabawassee River, farmers will pl...           23  \n",
       "804   .@PennLive Ed. Board: \"Democrats seem confiden...           84  \n",
       "494   Reports that President Biden will increase IRS...          251  \n",
       "924   For the guy who flew Neil Armstrong and Buzz A...         1813  \n",
       "466   The government doesn’t have a revenue problem,...          116  \n",
       "450   President Biden’s tax plan would put family fa...          421  \n",
       "959   Congratulations to my friend @SamanthaJPower o...           86  \n",
       "113   Reports that the Biden administration will ban...          144  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[[\"username\", \"name\", \"tweet\", \"likes_count\"]].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2. Geocoded Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take a break from politics, we'll look at tweets sent from near Deer District in Milwaukee on July 20 as up to 65,000 fans celebrated the Bucks' NBA title. The `geocode` argument in <tt>searchstr</tt> includes the longitude, latitude, and radius. This time, we aren't specifying a username/handle, and we aren't including an actual search term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = twint.Config()\n",
    "c.Hide_output = True\n",
    "c.Store_csv = True\n",
    "c.Output = \"geo_tweets.csv\"\n",
    "c.Limit = 1000\n",
    "searchstr = \"until:2021-07-21 since:2021-07-19 geocode:43.045110,-87.915820,5km\" # within 5km of Deer District\n",
    "c.Search = searchstr\n",
    "twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = pd.read_csv(\"geo_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df.date.min(), geo_df.date.max(), geo_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df[[\"username\", \"tweet\", \"likes_count\"]].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Reddit Content using `psaw`\n",
    "\n",
    "Another amazing resource for social media data is [pushshift.io](https://pushshift.io/), which archives vast amounts of data and makes it easily accessible. We'll use the [`psaw` library](https://github.com/dmarx/psaw) to access content from the pushshift.io Reddit API.\n",
    "\n",
    "For this example, we'll get posts to r/WallStreetBets from the last week of January, 2021. During this time, there was a lot of excitement about the rise of the GameStop stock--and then trading was halted on some platforms, [such as Robinhood](https://www.reuters.com/business/us-congress-hold-hearings-gamestop-trading-state-stock-markets-2021-01-28/).\n",
    "\n",
    "First, create an instance of the `PushShiftAPI()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the helper function <tt>get_results()</tt> to turn the results we get into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(subreddit: str, start_epoch, before_epoch, limit=10):\n",
    "    res = list(api.search_submissions(after=start_epoch,\n",
    "                                      before=before_epoch,\n",
    "                                      subreddit=subreddit,\n",
    "                                      limit=limit))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb = []\n",
    "\n",
    "year = 2020\n",
    "month = 1\n",
    "days = range(24,31)\n",
    "\n",
    "epochs = []\n",
    "\n",
    "for day in days:\n",
    "    start_epoch=int(dt.datetime(year, month, day).timestamp())\n",
    "    try:\n",
    "        before_epoch=int(dt.datetime(year, month, day+1).timestamp())\n",
    "    except:\n",
    "        before_epoch=int(dt.datetime(year, month+1, 1).timestamp()) # first day of next month\n",
    "        \n",
    "    epochs.append((start_epoch, before_epoch))\n",
    "    res = get_results(\"WallStreetBets\", start_epoch, before_epoch)\n",
    "    wsb.append(res)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_flat = [post for sublist in wsb for post in sublist] # turn list of lists into list of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_df = pd.DataFrame([post.d_ for post in wsb_flat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsb_df[[\"author\", \"title\", \"selftext\", \"score\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
