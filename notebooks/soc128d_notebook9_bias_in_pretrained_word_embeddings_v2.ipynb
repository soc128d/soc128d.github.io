{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc3f3220",
   "metadata": {},
   "source": [
    "#### Sociology 128D: Mining Culture Through Text Data: Introduction to Social Data Science – Summer '22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4c3ab2",
   "metadata": {},
   "source": [
    "# Notebook 9: Bias and Stereotyping in Pretrained Word Embeddings\n",
    "\n",
    "In this notebook, we will use pretrained word embeddings to develop intuitions about how word embedding models can be used to answer social research questions. We start with a brief introduction to word embeddings in general and a demonstration of solving analogies using word vectors.\n",
    "\n",
    "Next, we draw inspiration from [\"The Geometry of Culture\" (Kozlowski, Taddy, & Evans, 2019)](https://journals.sagepub.com/doi/full/10.1177/0003122419877135) and explore how word embedding models can be used to study cultural associations, for example classed, racialized, and gendered perceptions of music."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eada6e",
   "metadata": {},
   "source": [
    "## What are Word Embeddings?\n",
    "\n",
    "Word *vectors*, generally speaking, are vectors (lists of numbers) that represent the positions of distinct words in a shared embedding space. They are similar to the tf-idf-weighted document vectors we used in [Notebook 5](https://soc128d.github.io/notebooks/) in that the unit of analysis is represented by a vector of numbers and these numbers reflect information about the co-occurrence of words in a corpus. In the case of word vectors, however, the unit of analysis is the individual word, not a document.\n",
    "\n",
    "In general, word embeddings capture patterns of similar usage of words. Words used in similar contexts will be closer together in the embedding space. Synonyms are the extreme case: if you replace one synonym with another in a sentence (e.g., replace \"car\" with \"automobile\"), the meaning of the sentence may not change a great deal. Both \"car\" and \"automobile\" could reasonably be used in that sentence, and both versions of that sentence (or sentences like it) could occur frequently in a corpus. The other words in the sentence are <tt>context words</tt> (and, in fact, every word is a context word for every other word in a given sentence). Words used frequently in the presence of the same context words are assumed to be similar, and we want word vectors that reflect this.\n",
    "\n",
    "In Notebooks 3 and 5, we used a document X term (word) matrix to represent documents as vectors. To represent individual *words* as vectors, we could use that approach, but word vectors based on a document-term matrix would reflect the extent to which words are used in the same documents. If we want to represent the words as vectors based on how similarly they are used to other individual words, we want something like a word X _context word_ matrix representing the number of times words co-occur (or are \"context words\" for one another) in, say, individual sentences.\n",
    "\n",
    "The issue with a word X *context word* matrix is that such a matrix is often very sparse: it might be very large if we have a large vocabulary, and many words may never directly co-occur. Other approaches lead to \"dense vectors\" in which columns do not correspond to individual context words. Despite having a vocabulary of size V (which may be in the thousands or even millions), we often learn representations of words using only 50-300 dimensions. These lower-dimensional representations are typically what we refer to when we talk about **word embeddings**.\n",
    "\n",
    "We can use measures like cosine similarity to quantify the distance in the embedding space, and this reflects similarity in how words are used. We often make the further leap of assuming that this measures similarity in what words _mean_. [Sahlgren (2008, p. 1)](https://www.diva-portal.org/smash/get/diva2:1041938/FULLTEXT01.pdf) points to numerous formulations of this claim:\n",
    "\n",
    "> Distributional approaches to meaning acquisition utilize distributional properties of linguistic entities as the building blocks of semantics. In doing so, they rely fundamentally on a set of assumptions about the nature of language and meaning referred to as the distributional hypothesis. This hypothesis is often stated in terms like “words which are similar in meaning occur in similar contexts” (Rubenstein & Goodenough, 1965); “words with similar meanings will occur with similar neighbors if enough text material is available” (Sch ̈utze & Pedersen, 1995); “a representation that captures much of how words are used in natural context will capture much of what we mean by meaning” (Landauer & Dumais, 1997); and “words that occur in the same contexts tend to have similar meanings” (Pantel, 2005), just to quote a few representative examples. The general idea behind the distributional hypothesis seems clear enough: there is a correlation between distributional similarity and meaning similarity, which allows us to utilize the former in order to estimate the latter.\n",
    "\n",
    "There are a number of approaches to training word embeddings, but one of the most common–as we've seen in various readings–is word2vec. **You can find an excellent illustrated guide to word2vec [here](https://jalammar.github.io/illustrated-word2vec/).** In brief, the word2vec algorithm involves either iteratively predicting a target word based on the context words in a sentence or, alternatively, predicting the context words (and *not* predicting certain wrong words, or \"negative samples\") from the target word. With each iteration, representations of words are moved closer together if they are used in similar ways. Another common approach to training word embeddings is called [GloVe](https://nlp.stanford.edu/projects/glove/), and this is based more directly on a word X word co-occurrence matrix. More sophisticated approaches have been developed, and a lot of attention has been given to word embeddings from language models like BERT, but we are going to stick with word2vec and GloVe for this notebook.\n",
    "\n",
    "[Jurafsky and Martin (2021)](https://web.stanford.edu/~jurafsky/slp3/) have an excellent chapter on vector semantics and embeddings, which you can find [here](https://web.stanford.edu/~jurafsky/slp3/6.pdf).\n",
    "\n",
    "## Stereotyping and Bias in Word Embeddings\n",
    "\n",
    "Word embeddings are attempts to represent words as vectors such that words with similar meanings will have similar vectors. Word embeddings are learned based on how words are used in a corpus and, as a result, may be influenced by bias inherent to a given corpus. In general, however, stereotyping and bias are a part of natural language. From an engineering standpoint, we want to remove bias from word embeddings (or **debias** them) so that our applications do not perpetuate stereotypes or treat people in unfair ways. From a social science standpoint, however, we may want to explore stereotyping or bias directly.\n",
    "\n",
    "The classic example of a paper on detecting bias and debiasing word embeddings is [Bolukbasi et al. (2016)](https://proceedings.neurips.cc/paper/2016/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html). The title of the paper–\"Man is to Computer Programmer as Woman is to Homemaker?\"–says it all. As we'll see, you can quite effectively complete analogies using word embeddings, but completing those analogies may rely on assumptions about topics like gender. [Garg et al. (2018)](https://www.pnas.org/doi/full/10.1073/pnas.1720347115) offer another classic example of using word embeddings to study stereotyping and bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc86363",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You'll need a version of the `gensim` library. You can install `gensim` from the Anaconda interpreter using the following command:\n",
    "\n",
    "```\n",
    "conda install -c anaconda gensim\n",
    "```\n",
    "\n",
    "If you are not using Anaconda, you can use `pip` to install `gensim` from the command line:\n",
    "\n",
    "```\n",
    "pip3 install gensim\n",
    "```\n",
    "\n",
    "This notebook has been tested with versions 3.8.3, 4.0.1, and 4.1.2. In the shift from version 3 to version 4, [a few notable changes were made](https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4). For this notebook, the only difference that should matter is that word embedding models had a <tt>.vocab</tt> attribute in version 3, and this has been replaced with <tt>.key_to_index</tt> in version 4. There are a couple of lines of code in this notebook that use one of these attributes; they are flagged for you, and you can simply make sure to comment out the version you don't need and uncomment the version you do. If you're using version 4, use <tt>.key_to_index</tt>. If you're using an older version, use the lines of code with <tt>.vocab</tt>.\n",
    "\n",
    "Later in the notebook, you'll explore latent cultural dimensions measured as in [\"The Geometry of Culture\"](https://journals.sagepub.com/doi/full/10.1177/0003122419877135). You will create these using pairs of antonyms provided by the authors. The notebook links to these lists of words, and you can access them using either a function that will load them from your computer (if you've downloaded them) or a different function that will read them directly from the web. If you plan to use the word pairs on your own (e.g., for your project) or plan to run the notebook multiple times, it might be best to download the lists and use the function provided to load them. Otherwise, getting them directly from the web should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88929ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35969cd",
   "metadata": {},
   "source": [
    "`sklearn`'s `cosine_similarity()` method expects vectors to be in a certain shape. The <tt>cosine_reshape()</tt> function below is a helper function to handle that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c921a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_reshape(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Reshape word vectors and return cosine\n",
    "    similarity\n",
    "    \"\"\"\n",
    "    if vec1.shape[0] > 1:\n",
    "        vec1 = vec1.reshape(1, -1)\n",
    "    if vec2.shape[0] > 1:\n",
    "        vec2 = vec2.reshape(1, -1)\n",
    "    assert vec1.shape == vec2.shape\n",
    "    return cosine_similarity(vec1, vec2)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30093a7",
   "metadata": {},
   "source": [
    "Let's load our word embedding model. The notebook defaults to [<tt>word2vec-google-news-300</tt>](https://github.com/RaRe-Technologies/gensim-data), which has a vocabulary of three million words, has three hundred dimensions, and was trained on a Google New corpus. You can try other pretrained embeddings if you prefer. `gensim` also vectors trained using algorithms like [GloVe](https://nlp.stanford.edu/projects/glove/), with versions trained on different types of data and with different numbers of dimensions. Typically, smaller models (i.e., with smaller vocabularies or fewer dimensions) are faster to work with, but may not represent the meaning of the words as well.\n",
    "\n",
    "<b>Note: If you are using a particular model for the first time, the following cell will try to print a progress bar for the download, and you may get repeated warnings. It should continue to run despite these warnings. Once it has downloaded, you can run the cell again, or even comment it out, to hide the output.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors = gensim.downloader.load(\"glove-wiki-gigaword-300\") # if you'd prefer to try GloVe vectors instead!\n",
    "word_vectors = gensim.downloader.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb09276",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b2dc0",
   "metadata": {},
   "source": [
    "#### If you have `gensim` 4.0 and above, you'll need to comment out the lines with references to the <tt>.vocab</tt> attribute and uncomment the lines that reference to the <tt>.key_to_index</tt> attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2353647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db07080",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(word_vectors.key_to_index)\n",
    "# type(word_vectors.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c2bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(word_vectors.key_to_index):,} words in the vocabulary.\")\n",
    "# print(f\"There are {len(word_vectors.vocab):,} words in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e9891",
   "metadata": {},
   "source": [
    "Let's take a look at an example of a word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc06dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors[\"sociology\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d951e9c",
   "metadata": {},
   "source": [
    "The embedding for sociology indicates its position in a 300-dimensional space, but there is a vocabulary of three million words. The dimensions do not correspond to context words, and do not necessarily correspond to anything interpretable on their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a26af",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_vectors[\"sociology\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd64fd9",
   "metadata": {},
   "source": [
    "## Similarity and Analogies\n",
    "\n",
    "`gensim` makes it easy to compare word embeddings. the `.similarity()` method takes two words as arguments and provides the cosine similarity of the corresponding word embeddings. This is the same measure of similarity we used for comparing document vectors.\n",
    "\n",
    "`gensim`'s `.most_similar()` method takes one word as an argument and provides the <tt>topn</tt> most similar words in the model. However, the `.most_similar()` method also allows you to add and subtract word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba75a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.similarity(\"sociology\", \"anthropology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a135485",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(\"sociology\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7bfbd",
   "metadata": {},
   "source": [
    "The typical example of analogies with word vectors is finding a feminine counterpart of \"king\" by addition and subtraction of other vectors. Starting with the vector for king, we can subtract the vector for \"man\" and add the vector for \"woman.\"\n",
    "\n",
    "The analogy takes the following form:\n",
    "```\n",
    " man:king::woman:___\n",
    "```\n",
    "Using the `.most_similar()` method, we provide a `list` called <tt>positive</tt> that includes \"king\" and \"woman\" and a `list` called <tt>negative</tt> including only \"man.\" You can think of this as adding the vector for \"woman\" to the vector for \"king\" while subtracting the vector for \"man.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657094c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a0192a",
   "metadata": {},
   "source": [
    "Word vectors can also seemingly pick up aspects of grammar:\n",
    "\n",
    "```\n",
    "wide:wider::light:___\n",
    "read:reading::listen:___\n",
    "write:wrote::listen:___\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cf19f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=[\"wider\", \"light\"], negative=[\"wide\"], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c2939",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=[\"reading\", \"listen\"], negative=[\"read\"], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=[\"wrote\", \"listen\"], negative=[\"write\"], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851da09a",
   "metadata": {},
   "source": [
    "What about when we subtract the vector for \"society\" from the vector for \"sociology\" and add the vector for \"cell\"?\n",
    "\n",
    "```\n",
    "society:sociology::cell:___\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c0f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=[\"sociology\", \"cell\"], negative=[\"society\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60fb410",
   "metadata": {},
   "source": [
    "## Measuring \"Cultural Dimensions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ac2dd",
   "metadata": {},
   "source": [
    "#### If you would prefer to download and save the lists of word pairs used by Kozlowski et al. (2019):\n",
    "\n",
    "Affluence pairs: https://raw.githubusercontent.com/KnowledgeLab/GeometryofCulture/master/code/word_pairs/affluence_pairs.csv\n",
    "\n",
    "Gender pairs: https://raw.githubusercontent.com/KnowledgeLab/GeometryofCulture/master/code/word_pairs/gender_pairs.csv\n",
    "\n",
    "Race pairs: https://raw.githubusercontent.com/KnowledgeLab/GeometryofCulture/master/code/word_pairs/race_pairs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848aeebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pairs_from_file(f: str) -> list:\n",
    "    \"\"\"\n",
    "    Read in word pairs from `f` as list\n",
    "    \"\"\"\n",
    "    \n",
    "    pairs = open(f, \"r\").read().strip().split(\"\\n\")\n",
    "    pairs = [pair.split(\",\") for pair in pairs]\n",
    "    return [(pair[0], pair[1]) for pair in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89f8866",
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_f = \"affluence_pairs.csv\"\n",
    "gender_f = \"gender_pairs.csv\"\n",
    "race_f = \"race_pairs.csv\"\n",
    "\n",
    "aff_pairs = read_pairs_from_file(aff_f)\n",
    "gender_pairs = read_pairs_from_file(gender_f)\n",
    "race_pairs = read_pairs_from_file(race_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f278ab0c",
   "metadata": {},
   "source": [
    "#### If you would prefer to get the lists of word pairs used by Kozlowski et al. (2019) using the `requests` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c3df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pairs_from_url(url: str) -> list:\n",
    "    \"\"\"\n",
    "    Get word pairs using requests library\n",
    "    \"\"\"\n",
    "    \n",
    "    pairs = requests.get(url)\n",
    "    pairs = pairs.text.strip().split(\"\\n\")\n",
    "    pairs = [pair.replace(\"\\r\", \"\").split(\",\") for pair in pairs]\n",
    "    return [(pair[0], pair[1]) for pair in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1994c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_url = \"https://raw.githubusercontent.com/KnowledgeLab/GeometryofCulture/master/code/word_pairs/affluence_pairs.csv\"\n",
    "gender_url = \"https://raw.githubusercontent.com/KnowledgeLab/GeometryofCulture/master/code/word_pairs/gender_pairs.csv\"\n",
    "race_url = \"https://raw.githubusercontent.com/KnowledgeLab/GeometryofCulture/master/code/word_pairs/race_pairs.csv\"\n",
    "\n",
    "aff_pairs = read_pairs_from_url(aff_url)\n",
    "gender_pairs = read_pairs_from_url(gender_url)\n",
    "race_pairs = read_pairs_from_url(race_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb26a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e89802",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eefaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d64c55",
   "metadata": {},
   "source": [
    "The function below is a recreation of the <tt>make_dim()</tt> function in the original `R` code provided by Kozlowski et al. [here](https://github.com/KnowledgeLab/GeometryofCulture/blob/master/code/build_cultural_dimensions.R). Like other research on bias in word embeddings, this work presupposes cultural associations that can be represented as binaries or continua.\n",
    "\n",
    "To identify an \"affluence\" dimension, this approach uses the differences of vectors for antonyms like \"rich\" and \"poor\" (e.g., \"rich\" minus \"poor\").\n",
    "\n",
    "To identify \"gender\" and \"race\" dimensions, this approach makes the assumption that _culturally_ there are axes where the idea of gender or ethnic \"antonyms\" makes sense. If there is gender bias such that some things are stereotyped as being more associated with men than women (or vice versa), this approach may be able to detect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1450ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dim(word_pairs: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate a \"cultural dimension\" as in\n",
    "    https://github.com/KnowledgeLab/GeometryofCulture/blob/master/code/build_cultural_dimensions.R\n",
    "    \"\"\"\n",
    "    \n",
    "    diffs = []\n",
    "    \n",
    "    for pair in word_pairs:\n",
    "        \n",
    "        word1, word2 = pair\n",
    "        \n",
    "        if word1 in word_vectors and word2 in word_vectors:\n",
    "\n",
    "            vec1 = word_vectors[pair[0]]\n",
    "            vec1 = vec1/np.linalg.norm(vec1) # norm the first word vector\n",
    "\n",
    "            vec2 = word_vectors[pair[1]]\n",
    "            vec2 = vec2/np.linalg.norm(vec2) # norm the second second vector\n",
    "\n",
    "            diff = vec1 - vec2\n",
    "            diff = diff/np.linalg.norm(diff) # norm the difference between the vectors\n",
    "            diffs.append(diff)               # append the difference to the list\n",
    "            \n",
    "        else:\n",
    "            print(f\"Missing: {pair}\") # missing words from affluence pairs\n",
    "        \n",
    "    dim = np.mean(diffs, axis=0) # average the vectors appended to the list\n",
    "    dim = dim/np.linalg.norm(dim) # norm the average\n",
    "    \n",
    "    return dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c8a3cc",
   "metadata": {},
   "source": [
    "Now we can calculate the cultural dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd63019",
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_dim = make_dim(aff_pairs)\n",
    "gender_dim = make_dim(gender_pairs)\n",
    "race_dim = make_dim(race_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1042620",
   "metadata": {},
   "source": [
    "Each is a 300-dimensional vector just like the word vectors, but it doesn't correspond to a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039dcece",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(aff_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0635787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_dim.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5771114",
   "metadata": {},
   "source": [
    "#### Locating musical genres along these dimensions\n",
    "\n",
    "Now we can create a plot like Figure 3 in the original paper (p. 921). We won't overlay a plot based on survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453a0d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = [\"jazz\", \"rap\", \"opera\", \"punk\", \"techno\", \"hiphop\", \"bluegrass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75cadde",
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_proj_dict = {}\n",
    "gender_proj_dict = {}\n",
    "race_proj_dict = {}\n",
    "\n",
    "for genre in genres:\n",
    "    cos_aff = cosine_reshape(word_vectors[genre], aff_dim)\n",
    "    aff_proj_dict[genre] = cos_aff\n",
    "    \n",
    "    cos_gender = cosine_reshape(word_vectors[genre], gender_dim)\n",
    "    gender_proj_dict[genre] = cos_gender\n",
    "    \n",
    "    cos_race = cosine_reshape(word_vectors[genre], race_dim)\n",
    "    race_proj_dict[genre] = cos_race"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a8197b",
   "metadata": {},
   "source": [
    "The results are dictionaries assigning a position to each musical genre along a particular dimension.\n",
    "\n",
    "For the affluence dimension, higher values indicate the words are closer to the more affluent side.\n",
    "\n",
    "For the gender dimension, higher values indicate words are more associated with men or masculinity (relative to women or femininity).\n",
    "\n",
    "For the race dimension, higher values indicate words are potentially more stereotyped as Black (relative to white)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b1b09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_proj_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721cc83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_proj_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a86bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "race_proj_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3794f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for genre in genres:\n",
    "    plt.text(x=race_proj_dict[genre], y=aff_proj_dict[genre], s=genre)\n",
    "\n",
    "# These are set to work with the word2vec embeddings. You may need to change the limits of the axes if you use GloVe.\n",
    "plt.xlim(-0.05,0.3)\n",
    "plt.ylim(-0.10, 0.15)\n",
    "\n",
    "plt.xlabel(\"Race Projection\")\n",
    "plt.ylabel(\"Class Projection\")\n",
    "plt.title(\"Class and Racial Associations of Musical Genres\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfbfb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for genre in genres:\n",
    "    plt.text(x=race_proj_dict[genre], y=gender_proj_dict[genre], s=genre)\n",
    "    \n",
    "plt.xlim(-0.05,0.3)\n",
    "plt.ylim(-0.10, 0.15)\n",
    "\n",
    "plt.xlabel(\"Race Projection\")\n",
    "plt.ylabel(\"Gender Projection\")\n",
    "plt.title(\"Gender and Racial Associations of Musical Genres\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89070e6",
   "metadata": {},
   "source": [
    "Importantly, Kozlowski et al. (2019) compare their results to survey data and find similar associations between musical genres and these projections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5ae3e1",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 1</b><br><br>\n",
    "    For this exercise, you will identify another domain of social life that may exhibit the kind of bias we see above.<br><br>\n",
    "    1.1 Create a list of things associated with the domain you have identified, just like the list of musical genres above (<tt>genres</tt>). Only include words that are in the vocabulary of the word embedding model you are using.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f30cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea11f4d",
   "metadata": {},
   "source": [
    "You can use the `filter()` method and a lambda function to get rid of any words that aren't in the word embedding model. Assuming your word embedding model is named <tt>word_vectors</tt> (as above) and your list is called <tt>list_of_things</tt> (which is optional), the following line would keep only the words that are in the word embedding model's vocabulary.\n",
    "\n",
    "```python\n",
    "list_of_things = list(filter(lambda x: x in word_vectors, list_of_things))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e493b2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    1.2 What domain did you choose? Why? What associations do you expect to see with the cultural dimensions we've measured?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7e8d48",
   "metadata": {},
   "source": [
    "_Your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe729ba",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    1.3 Replace <tt>LIST_OF_THINGS</tt> with the name of your list and run the cell below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf7262",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in LIST_OF_THINGS: # replace LIST_OF_THINGS\n",
    "    cos_aff = cosine_reshape(word_vectors[element], aff_dim)\n",
    "    aff_proj_dict[element] = cos_aff\n",
    "    \n",
    "    cos_gender = cosine_reshape(word_vectors[element], gender_dim)\n",
    "    gender_proj_dict[element] = cos_gender\n",
    "    \n",
    "    cos_race = cosine_reshape(word_vectors[element], race_dim)\n",
    "    race_proj_dict[element] = cos_race"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a51e5a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    1.4 Now plot the positions of the words you chose in a two-dimensional space defined by two of the three cultural dimensions we've measured. (The code in the cells with the plots above should only need minor tweaks to plot your words.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f821ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bfd7e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    1.5 Do the results match the associations you predicted in 1.2?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3a2edc",
   "metadata": {},
   "source": [
    "_Your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0d9a1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Exercise 2</b><br><br>\n",
    "    For this exercise, you will identify and measure a new cultural dimension.<br><br>\n",
    "    2.1 Begin by identifying pairs of words that should be at opposite ends of the dimension. These may be antonyms like \"rich\" and \"poor\" or they may be anchors for a cultural dimension related to stereotyping and bias that aren't necessarily antonyms. (For example, \"Black\" and \"White\" as ethnicities aren't antonyms, but are useful anchors for identifying a cultural dimension that allows us to measure and visualize bias.) <br><br>\n",
    "    You can create this list of word pairs as a list of lists or list of tuples inside the notebook, or you might try saving them in a CSV file and opening them with the <tt>read_pairs_from_file()</tt> function defined above. NOTE: The anchors should always be in the same order. For example, in <tt>affluence_pairs.csv</tt>, words associated with the affluent end of the spectrum are always listed first, and the antonyms are listed the second. You can look at the CSVs used above for guidance.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e163ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c151fe",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    2.2 What is your cultural dimension meant to measure? What kinds of stereotypes might you see?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7edf7",
   "metadata": {},
   "source": [
    "_Your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cabb03",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    2.3 Calculate the cultural dimension using the <tt>make_dim()</tt> function defined above. This should look like the following (but with your word pairs, and a name other than <tt>aff_dim</tt>):\n",
    "</div>\n",
    "\n",
    "```python\n",
    "aff_dim = make_dim(aff_pairs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe357d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85fe1bf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    2.4 Now using your dimension and one of the others, plot a list of words in the two-dimensional space defined by the dimensions like in 1.4.<br><br>\n",
    "    - Rename <tt>YOUR_DIM_PROJ_DICT</tt><br>\n",
    "    - Replace <tt>LIST_OF_THINGS</tt> with the list you want to use (which could be musical genres, the list you used in Exercise 1, or something else)<br>\n",
    "    - Replace <tt>YOUR_DIM</tt> with the name of the variable to which you've assigned your cultural dimension<br>\n",
    "    - Replace <tt>YOUR_DIM_PROJ_DICT</tt> in the last line with the name you chose in the first line<br>\n",
    "    - Create a plot using your dimension and one other dimension\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de19203",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_DIM_PROJ_DICT = {} # rename this\n",
    "\n",
    "for element in LIST_OF_THINGS: # replace LIST_OF_THINGS\n",
    "    cos_aff = cosine_reshape(word_vectors[element], aff_dim)\n",
    "    aff_proj_dict[element] = cos_aff\n",
    "    \n",
    "    cos_gender = cosine_reshape(word_vectors[element], gender_dim)\n",
    "    gender_proj_dict[element] = cos_gender\n",
    "    \n",
    "    cos_race = cosine_reshape(word_vectors[element], race_dim)\n",
    "    race_proj_dict[element] = cos_race\n",
    "    \n",
    "    cos_new = cosine_reshape(word_vectors[element], YOUR_DIM) # replace YOUR_DIM with the name of your cultural dim.\n",
    "    YOUR_DIM_PROJ_DICT[element] = cos_new # replace the name of the dictionary with the name you chose above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b201d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR PLOT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b21e6b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    2.5 Do the results match the associations you predicted in 2.2?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f79eed3",
   "metadata": {},
   "source": [
    "_Your answer here_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
